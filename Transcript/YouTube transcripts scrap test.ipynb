{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eded7772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d3b60bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install webdriver-manager --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50d9a1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So first, let's start with introducing the instructors. So for me, I'm currently on a temporary deferral from the PhD\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "# Set up Chrome options and service\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Runs Chrome in headless mode\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "service = Service(ChromeDriverManager().install())\n",
    "\n",
    "# Set up WebDriver\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Open YouTube video\n",
    "video_url = 'https://www.youtube.com/watch?v=XfpMkf4rD6E&list=PLpSmuCX65Aw8BbLqzdAB7aAcQR75BhMjz&index=43&t=589s'\n",
    "driver.get(video_url)\n",
    "\n",
    "# Wait for page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Scroll to the description section\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "time.sleep(2)\n",
    "\n",
    "# Expand the description if necessary (YouTube may hide part of the description)\n",
    "try:\n",
    "    more_button = driver.find_element(By.XPATH, '//*[@id=\"expand\"]')\n",
    "    more_button.click()\n",
    "    time.sleep(1)\n",
    "except Exception as e:\n",
    "    print(\"No 'More' button found or already expanded.\")\n",
    "\n",
    "# Wait for the transcript option to be visible and click it\n",
    "time.sleep(1)\n",
    "transcript_button = driver.find_element(By.XPATH, '//*[@id=\"primary-button\"]/ytd-button-renderer/yt-button-shape/button')\n",
    "transcript_button.click()\n",
    "\n",
    "# Wait for the transcript to load\n",
    "time.sleep(2)\n",
    "\n",
    "# Locate and extract the transcript\n",
    "transcript_elements = driver.find_elements(By.XPATH, '//*[@id=\"segments-container\"]/ytd-transcript-segment-renderer[9]/div/yt-formatted-string')\n",
    "transcript = \"\"\n",
    "for element in transcript_elements:\n",
    "    transcript += element.text + \"\\n\"\n",
    "\n",
    "# Print the transcript\n",
    "print(transcript)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e4ee93e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\".//yt-formatted-string[@class=\"segment-text\"]\"}\n  (Session info: chrome=128.0.6613.138); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n0   chromedriver                        0x0000000102d95208 cxxbridge1$str$ptr + 1927396\n1   chromedriver                        0x0000000102d8d66c cxxbridge1$str$ptr + 1895752\n2   chromedriver                        0x0000000102988808 cxxbridge1$string$len + 89564\n3   chromedriver                        0x00000001029ccbcc cxxbridge1$string$len + 369056\n4   chromedriver                        0x00000001029c3064 cxxbridge1$string$len + 329272\n5   chromedriver                        0x0000000102a06228 cxxbridge1$string$len + 604156\n6   chromedriver                        0x00000001029c1698 cxxbridge1$string$len + 322668\n7   chromedriver                        0x00000001029c2310 cxxbridge1$string$len + 325860\n8   chromedriver                        0x0000000102d5be78 cxxbridge1$str$ptr + 1693012\n9   chromedriver                        0x0000000102d6077c cxxbridge1$str$ptr + 1711704\n10  chromedriver                        0x0000000102d413ec cxxbridge1$str$ptr + 1583816\n11  chromedriver                        0x0000000102d6104c cxxbridge1$str$ptr + 1713960\n12  chromedriver                        0x0000000102d31fc8 cxxbridge1$str$ptr + 1521316\n13  chromedriver                        0x0000000102d7eb68 cxxbridge1$str$ptr + 1835588\n14  chromedriver                        0x0000000102d7ece4 cxxbridge1$str$ptr + 1835968\n15  chromedriver                        0x0000000102d8d308 cxxbridge1$str$ptr + 1894884\n16  libsystem_pthread.dylib             0x00000001805bef94 _pthread_start + 136\n17  libsystem_pthread.dylib             0x00000001805b9d34 thread_start + 8\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m transcript \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m transcript_elements:\n\u001b[0;32m---> 50\u001b[0m     text \u001b[38;5;241m=\u001b[39m element\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mXPATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.//yt-formatted-string[@class=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegment-text\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m     51\u001b[0m     transcript \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m text \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Print the entire transcript\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/selenium/webdriver/remote/webelement.py:417\u001b[0m, in \u001b[0;36mWebElement.find_element\u001b[0;34m(self, by, value)\u001b[0m\n\u001b[1;32m    414\u001b[0m     by \u001b[38;5;241m=\u001b[39m By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR\n\u001b[1;32m    415\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 417\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute(Command\u001b[38;5;241m.\u001b[39mFIND_CHILD_ELEMENT, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing\u001b[39m\u001b[38;5;124m\"\u001b[39m: by, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: value})[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/selenium/webdriver/remote/webelement.py:395\u001b[0m, in \u001b[0;36mWebElement._execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    393\u001b[0m     params \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    394\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent\u001b[38;5;241m.\u001b[39mexecute(command, params)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:354\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    352\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n\u001b[1;32m    355\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\".//yt-formatted-string[@class=\"segment-text\"]\"}\n  (Session info: chrome=128.0.6613.138); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n0   chromedriver                        0x0000000102d95208 cxxbridge1$str$ptr + 1927396\n1   chromedriver                        0x0000000102d8d66c cxxbridge1$str$ptr + 1895752\n2   chromedriver                        0x0000000102988808 cxxbridge1$string$len + 89564\n3   chromedriver                        0x00000001029ccbcc cxxbridge1$string$len + 369056\n4   chromedriver                        0x00000001029c3064 cxxbridge1$string$len + 329272\n5   chromedriver                        0x0000000102a06228 cxxbridge1$string$len + 604156\n6   chromedriver                        0x00000001029c1698 cxxbridge1$string$len + 322668\n7   chromedriver                        0x00000001029c2310 cxxbridge1$string$len + 325860\n8   chromedriver                        0x0000000102d5be78 cxxbridge1$str$ptr + 1693012\n9   chromedriver                        0x0000000102d6077c cxxbridge1$str$ptr + 1711704\n10  chromedriver                        0x0000000102d413ec cxxbridge1$str$ptr + 1583816\n11  chromedriver                        0x0000000102d6104c cxxbridge1$str$ptr + 1713960\n12  chromedriver                        0x0000000102d31fc8 cxxbridge1$str$ptr + 1521316\n13  chromedriver                        0x0000000102d7eb68 cxxbridge1$str$ptr + 1835588\n14  chromedriver                        0x0000000102d7ece4 cxxbridge1$str$ptr + 1835968\n15  chromedriver                        0x0000000102d8d308 cxxbridge1$str$ptr + 1894884\n16  libsystem_pthread.dylib             0x00000001805bef94 _pthread_start + 136\n17  libsystem_pthread.dylib             0x00000001805b9d34 thread_start + 8\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "# Set up Chrome options and service\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Runs Chrome in headless mode\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "service = Service(ChromeDriverManager().install())\n",
    "\n",
    "# Set up WebDriver\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Open YouTube video\n",
    "video_url = 'https://www.youtube.com/watch?v=XfpMkf4rD6E&list=PLpSmuCX65Aw8BbLqzdAB7aAcQR75BhMjz&index=43&t=589s'\n",
    "driver.get(video_url)\n",
    "\n",
    "# Wait for page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Scroll to the description section\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "time.sleep(2)\n",
    "\n",
    "# Expand the description if necessary (YouTube may hide part of the description)\n",
    "try:\n",
    "    more_button = driver.find_element(By.XPATH, '//*[@id=\"expand\"]')\n",
    "    more_button.click()\n",
    "    time.sleep(1)\n",
    "except Exception as e:\n",
    "    print(\"No 'More' button found or already expanded.\")\n",
    "\n",
    "# Wait for the transcript option to be visible and click it\n",
    "time.sleep(1)\n",
    "transcript_button = driver.find_element(By.XPATH, '//*[@id=\"primary-button\"]/ytd-button-renderer/yt-button-shape/button')\n",
    "transcript_button.click()\n",
    "\n",
    "# Wait for the transcript to load\n",
    "time.sleep(2)\n",
    "\n",
    "# Locate and extract all transcript segments\n",
    "transcript_elements = driver.find_elements(By.XPATH, '//*[@id=\"segments-container\"]//ytd-transcript-segment-renderer')\n",
    "\n",
    "transcript = \"\"\n",
    "for element in transcript_elements:\n",
    "    text = element.find_element(By.XPATH, './/yt-formatted-string[@class=\"segment-text\"]').text\n",
    "    transcript += text + \"\\n\"\n",
    "\n",
    "# Print the entire transcript\n",
    "print(transcript)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3307692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "//*[@id=\"segments-container\"]/ytd-transcript-segment-renderer[1]\n",
    "//*[@id=\"segments-container\"]/ytd-transcript-segment-renderer[1]/div/yt-formatted-string\n",
    "//*[@id=\"segments-container\"]/ytd-transcript-segment-renderer[2]\n",
    "//*[@id=\"segments-container\"]/ytd-transcript-segment-renderer[2]/div/yt-formatted-string\n",
    "//*[@id=\"segments-container\"]/ytd-transcript-segment-renderer[3]\n",
    "//*[@id=\"segments-container\"]/ytd-transcript-segment-renderer[3]/div/yt-formatted-string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d597abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, everyone. Welcome to CS 25 Transformers United V2. This was a course that was held at Stanford\n",
      "in the winter of 2023. This course is not about robots that can transform into cars as this picture might suggest.\n",
      "Rather, it's about deep learning models that have taken the world by storm and have revolutionized the field of AI and others.\n",
      "Starting from natural language processing, transformers have been applied all over, computer vision, reinforcement learning, biology, robotics,\n",
      "et cetera. We have an exciting set of videos lined up for you with some truly fascinating speakers, talks, presenting\n",
      "how they're applying transformers to the research in different fields and areas.\n",
      "We hope you'll enjoy and learn from these videos. So without any further ado, let's get started.\n",
      "This is a purely introductory lecture. And we'll go into the building blocks of transformers.\n",
      "So first, let's start with introducing the instructors. So for me, I'm currently on a temporary deferral from the PhD\n",
      "program, and I'm leading AI at a robotics startup, Collaborative Robotics, that are working on some general purpose robots,\n",
      "somewhat like [INAUDIBLE]. And I'm very passionate about robotics and building FSG learning algorithms.\n",
      "My research interests are in reinforcement learning, computer vision, and remodeling, and I have a bunch of publications in robotics,\n",
      "autonomous driving, and other areas. My undergrad was at Cornell. If someone is from Cornell, so nice to [INAUDIBLE]..\n",
      "So I'm Stephen, currently a first-year CS PhD here. Previously did my master's at CMU and undergrad at Waterloo.\n",
      "I'm mainly into NLP research, anything involving language and text, but more recently, I've\n",
      "been getting more into computer vision as well as [INAUDIBLE] And just some stuff I do for fun, a lot of music\n",
      "stuff, mainly piano. Some self-promo of what I post a lot on my Insta, YouTube, and TikTok, so if you guys want to check it out.\n",
      "My friends and I are also starting a Stanford piano club, so if anybody's interested, feel free to email\n",
      "or DM me for details. Other than that, martial arts, bodybuilding, and huge fan\n",
      "of k-dramas, anime, and occasional gamer. [LAUGHS]\n",
      "OK, cool. Yeah, so my name is Rylan. Instead of talking about myself, I just want to very briefly say that I'm super excited to take this class.\n",
      "I took it the last time-- sorry-- to teach this. Excuse me. I took it the last time I was offered. I had a bunch of fun.\n",
      "I thought we brought in a really great group of speakers last time. I'm super excited for this offering.\n",
      "And yeah, I'm thankful that you're all here, and I'm looking forward to a really fun quarter together. Thank you. Yeah, so fun fact, Rylan was the most outspoken student\n",
      "last year. And so if someone wants to become an instructor next year, you know what to do. [LAUGHTER]\n",
      "OK, cool. Let's see. OK, I think we have a few minutes.\n",
      "So what we hope you will learn in this class is, first of all, how do transformers work, how they\n",
      "are being applied, just beyond NLP, and nowadays, like they are pretty [INAUDIBLE] them everywhere in AI machine learning.\n",
      "And what are some new and interesting directions of research in these topics.\n",
      "Cool, so this class is just an introductory. So we're just talking about the basics of transformers, introducing them, talking about the self-attention mechanism\n",
      "on which they're founded. And we'll do a deep dive more on models like BERT\n",
      "to GPT, stuff like that. So with that, happy to get started. OK, so let me start with presenting the attention\n",
      "timeline. Attention all started with this one paper. [INAUDIBLE] by Vaswani et al in 2017.\n",
      "That was the beginning of transformers. Before that, we had the prehistoric error,\n",
      "where we had models like RNM, LSDMs, and simple attention mechanisms that didn't work\n",
      "or [INAUDIBLE]. Starting 2017, we saw this explosion of transformers\n",
      "into NLP, where people started using it for everything. I even heard this quote from Google.\n",
      "It's like our performance increased every time we [INAUDIBLE] [CHUCKLES]\n",
      "For the [INAUDIBLE] after 2018 to 2020, we saw this explosion of transformers into other fields like vision, a bunch of other stuff,\n",
      "and like biology as a whole. And in last year, 2021 was the start of the generative era, where we got a lot of genetic modeling,\n",
      "started models like Codex, GPT, DALL-E, stable diffusions, or a lot of things\n",
      "happening in genetic modeling. And we started scaling up in AI.\n",
      "And now, the present. So this is 2022 and the startup in '23.\n",
      "And now we have models like ChatGPT, Whisperer, a bunch of others.\n",
      "And we're scaling onwards without splitting up, so that's great. So that's the future.\n",
      "So going more into this, so once there were RNNs.\n",
      "So we had Seq2Seq models, LSTMs, GRU. What worked there was that they were good at encoding history,\n",
      "but what did not work was they didn't encode long sequences and they were very bad at encoding context.\n",
      "So consider this example. Consider trying to predict the last word in the text,\n",
      "\"I grew up in France, dot, dot, dot. I speak fluent Dutch.\" Here, you need to understand the context for it\n",
      "to predict French, and attention mechanism is very good at that, whereas if they're just using LSDMs,\n",
      "it doesn't here work that well. Another thing transformers are good at is,\n",
      "more based on content, is also context prediction is like finding attention maps.\n",
      "If I have something like a word like it, what noun does it correlate to.\n",
      "And we can give a property attention on one of the possible activations.\n",
      "And this works better than existing mechanisms.\n",
      "OK, so where we were in 2021, we were on the verge of takeoff.\n",
      "We were starting to realize the potential of transformers in different fields. We solved a lot of long sequence problems\n",
      "like protein folding, AlphaFold, offline RL.\n",
      "We started to see few-shots, zero-shot generalization. We saw multimodal tasks and applications\n",
      "like generating images from language. So that was DALL-E. And it feels like [INAUDIBLE]..\n",
      "And this was also a talk on transformers that you can watch on YouTube. Yeah, cool.\n",
      "And this is where we were going from 2021 to 2022, which is we have gone from the version of [INAUDIBLE]\n",
      "And now, we are seeing unique applications in audio generation, art, music, storytelling. We are starting to see these new capabilities\n",
      "like commonsense, logical reasoning, mathematical reasoning. We are also able to now get human enlightenment\n",
      "and interaction. They're able to use reinforcement learning and human feedback. That's how ChatGPT is trained to perform really good.\n",
      "We have a lot of mechanisms for controlling toxicity bias and ethics now. And there are a lot of also, a lot\n",
      "of developments in other areas like diffusion models. Cool.\n",
      "So the future is a spaceship, and we are all excited about it.\n",
      "And there's a lot of more applications that we can enable, and it'll be great\n",
      "if you can see transformers also up there. One big example is video understanding and generation.\n",
      "That is something that everyone is interested in, and I'm hoping we'll see a lot of models in this area this year, also, finance, business.\n",
      "I'll be very excited to see GPT author a novel, but we need to solve very long sequence modeling.\n",
      "And most transformer models are still limited to 4,000 tokens or something like that. So we need to make them generalize much more\n",
      "better on long sequences. We also want to have generalized agents\n",
      "that can do a lot of multitask, a multi-input predictions\n",
      "like Gato. And so I think we will see more of that, too. And finally, we also want domain specific models.\n",
      "So you might want a GPT model, let's put it like maybe your health. So that could be like a DoctorGPT model.\n",
      "You might have a LawyerGPT model that's trained on only law data. So currently, we have GPT models that are trained on everything.\n",
      "But we might start to see more niche models that are good at one task. And we could have a mixture of experts,\n",
      "so it's like, you can think this is a-- how you'd normally consult an expert, you'll have expert AI models.\n",
      "And you can go to a different AI model for your different needs. There are still a lot of missing ingredients\n",
      "to make this all successful. The first of all is external memory.\n",
      "We are already starting to see this with the models like ChatGPT, where the inflections are short-lived.\n",
      "There's no long-term memory, and they don't have ability to remember or store conversations for long-term.\n",
      "And this is something you want to fix. Second is reducing the computation complexity.\n",
      "So attention mechanism is quadratic over the sequence length, which is slow. And we want to reduce it and make it faster.\n",
      "Another thing we want to do is we want to enhance the controllability of these models like a lot of these models can be stochastic.\n",
      "And we want to be able to control what sort of outputs we get from them. And you might have experienced the ChatGPT,\n",
      "if you just refresh, you get different output each time. But you might want to have a mechanism that controls what sort of things you get.\n",
      "And finally, we want to align our state of art language models with how the human brain works.\n",
      "And we are seeing the surge, but we still need more research on seeing how they can make more informed.\n",
      "Thank you. Great, hi. Yes, I'm excited to be here.\n",
      "I live very nearby, so I got the invites to come to class. And I was like, OK, I'll just walk over.\n",
      "But then I spent like 10 hours on the slides, so it wasn't as simple. So yeah, I'm going to talk about transformers.\n",
      "I'm going to skip the first two over there. I'm not going to talk about those. We'll talk about that one just to simplify the lecture\n",
      "since we don't have time. OK, so I wanted to provide a little bit of context\n",
      "on why does this transformers class even exist. So a little bit of historical context. I feel like Bilbo over there.\n",
      "I joined like telling you guys about this. I don't know if you guys saw Lord of the Rings.\n",
      "And basically, I joined AI in roughly 2012, the full course, so maybe a decade ago.\n",
      "And back then, you wouldn't even say that you joined AI by the way. That was like a dirty word. Now, it's OK to talk about, but back then, it\n",
      "was not even deep learning. It was machine learning. That was the term we would use if you were serious. But now, now, AI is OK to use, I think.\n",
      "So basically, do you even realize how lucky you are potentially entering this area in roughly 2023?\n",
      "So back then, in 2011 or so when I was working specifically on computer vision, your pipeline's looked like this.\n",
      "So you wanted to classify some images, you would go to a paper, and I think this is representative. You would have three pages in the paper describing\n",
      "all kinds of a zoo, of kitchen sink, of different kinds of features, descriptors. And you would go to a poster session\n",
      "and in computer vision conference, and everyone would have their favorite feature descriptor that they're proposing. And it's totally ridiculous, and you\n",
      "would take notes on which one you should incorporate into your pipeline because you would extract all of them, and then you would put an SVM on top.\n",
      "So that's what you would do. So there's two pages. Make sure you get your [? Spar ?] SIFT histograms, your SSIMs, your color histograms, textiles,\n",
      "tiny images. And don't forget the geometry specific histograms. All of them have basically complicated code by themselves.\n",
      "So you're collecting code from everywhere and running it, and it was a total nightmare. So on top of that, it also didn't work.\n",
      "[LAUGHTER] So this would be, I think, it represents the prediction from that time. You would just get predictions like this once in a while,\n",
      "and you'd be like, you just shrug your shoulders like that just happens once in a while. Today, you would be looking for a bug.\n",
      "And worse than that, every single chunk of AI\n",
      "had their own completely separate vocabulary that they work with. So if you go to NLP papers, those papers\n",
      "would be completely different. So you're reading the NLP paper, and you're like, what is this part of speech tagging,\n",
      "morphological analysis, and tactic parsing, co-reference resolution? What is MPBTKJ?\n",
      "And you're confused. So the vocabulary and everything was completely different. And you couldn't read papers, I would say, across different areas.\n",
      "So now, that changed a little bit starting 2012 when Al Krizhevsky and colleagues basically\n",
      "demonstrated that if you scale a large neural network on large data set, you can get very strong performance.\n",
      "And so up till then, there was a lot of focus on algorithms. But this showed that actually neural nets scale very well. So you need to now worry about compute and data,\n",
      "and you can scale it up. It works pretty well. And then that recipe actually did copy paste across many areas of AI.\n",
      "So we start to see neural networks pop up everywhere since 2012. So we saw them in computer vision, and NLP, and speech,\n",
      "and translation in RL and so on. So everyone started to use the same kind of modeling toolkit, modeling framework.\n",
      "And now when you go to NLP, and you start reading papers there, in machine translation, for example, this is a sequence to sequence paper\n",
      "which we'll come back to in a bit. You start to read those papers, and you're like, OK, I can recognize these words.\n",
      "Like there's a neural network. There's some parameters. There's an optimizer, and it starts to read things that you know of.\n",
      "So that decreased tremendously the barrier to entry across the different areas.\n",
      "And then, I think, the big deal is that when the transformer came out in 2017, it's not even that just the tool kits and the neural networks\n",
      "were similar-- there's that literally the architectures converged to like one architecture that you copy paste across everything seemingly.\n",
      "So this was kind of an unassuming machine translation paper at the time, proposing to transformer architecture. But what we found since then is that you can just basically\n",
      "copy paste this architecture and use it everywhere. And what's changing is the details of the data,\n",
      "and the chunking of the data, and how you feed it in. And that's a caricature, but it's kind of like a correct first order statement.\n",
      "And so now, papers are even more similar looking because everyone's just using transformer. And so this convergence was remarkable to watch\n",
      "and unfolded over the last decade. And it's pretty crazy to me. What I find interesting is I think\n",
      "this is some kind of a hint that we're maybe converging to something that maybe the brain is doing because the brain is very homogeneous and uniform\n",
      "across the entire sheet of your cortex. And OK, maybe some of the details are changing, but those feel like hyperparameters\n",
      "like a transformer. But your auditory cortex and your visual cortex and everything else looks very similar. And so maybe we're converging to some kind\n",
      "of a uniform powerful learning algorithm here. Something like that, I think, is interesting and exciting.\n",
      "OK, so I want to talk about where the transformer came from briefly, historically. So I want to start in 2003.\n",
      "I like this paper quite a bit. It was the first popular application of neural networks\n",
      "to the problem of language modeling, so predicting in this case, the next word in the sequence, which allows you to build generative models over text.\n",
      "And in this case, they were using multi-layer perceptron, so very simple neural net. The neural nets took three words and predicted the probability\n",
      "distribution for the fourth word in a sequence. So this was well and good at this point.\n",
      "Now, over time, people started to apply this to machine translation. So that brings us to sequence to sequence paper\n",
      "from 2014 that was pretty influential, and the big problem here was OK, we don't just want to take three words and predict the fourth.\n",
      "We want to predict how to go from an English sentence to a French sentence. And the key problem was OK, you can\n",
      "have arbitrary number of words in English and arbitrary number of words in French, so how do you get an architecture that can process\n",
      "this variably sized input? And so here they used a LSDM, and there's basically\n",
      "two chunks of this, which are covered by the slack, by this.\n",
      "But basically have an encoder LSDM on the left, and it just consumes one word at a time\n",
      "and builds up a context of what it has read. And then that acts as a conditioning vector to the decoder RNN or LSDM.\n",
      "That basically goes chonk, chonk, chonk for the next word in a sequence, translating the English to French or something like that.\n",
      "Now, the big problem with this, that people identified, I think, very quickly and tried to resolve is that there's what's called this encoder bottleneck.\n",
      "So this entire English sentence that we are trying to condition on is packed into a single vector that goes from the encoder to the decoder.\n",
      "And so this is just too much information to potentially maintain in a single vector, and that didn't seem correct. And so people who are looking around for ways\n",
      "to alleviate the attention of the encoder bottleneck as it was called at the time. And so that brings us to this paper,\n",
      "Neural Machine Translation by Jointly Learning to Align and Translate. And here, just quoting from the abstract, \"in this paper,\n",
      "we conjectured that the use of a fixed length vector is a bottleneck in improving the performance of the basic encoder-decoder architecture\n",
      "and propose to extend this by allowing the model to automatically soft search for parts of the source sentence that are relevant to predicting\n",
      "a target word without having to form these parts or hard segments exclusively.\"\n",
      "So this was a way to look back to the words that are coming from the encoder.\n",
      "And it was achieved using this soft search. So as you are decoding in the words\n",
      "here, while you are decoding them, you are allowed to look back at the words at the encoder via this soft attention mechanism proposed\n",
      "in this paper. And so this paper, I think, is the first time that I saw, basically, attention.\n",
      "So your context vector that comes from the encoder is a weighted sum of the hidden states\n",
      "of the words in the encoding. And then the weights of this sum come\n",
      "from a softmax that is based on these compatibilities between the current state as you're decoding\n",
      "and the hidden states generated by the encoder. And so this is the first time that really you start to look at it, and this is the current modern equations\n",
      "of the attention. And I think this was the first paper that I saw it in. It's the first time that there's a word\n",
      "attention used, as far as I know, to call this mechanism. So I actually tried to dig into the details of the history\n",
      "of the attention. So the first author here, Dzmitry, I had an email correspondence with him,\n",
      "and I basically sent him an email. I'm like, Dzmitry, this is really interesting. Just rumors have taken over. Where did you come up with the soft attention\n",
      "mechanism that ends up being the heart of the transformer? And to my surprise, he wrote me back this massive email, which\n",
      "was really fascinating. So this is an excerpt from that email.\n",
      "So basically, he talks about how he was looking for a way to avoid this bottleneck between the encoder and decoder.\n",
      "He had some ideas about cursors that traverse the sequences that didn't quite work out. And then here, \"so one day, I had this thought\n",
      "that it would be nice to enable the decoder RNN to learn to search where to put the cursor in the source sequence.\n",
      "This was sort of inspired by translation exercises that learning English in my middle school involved.\n",
      "Your gaze shifts back and forth between source and target, sequence as you translate.\" So literally, I thought that this was kind of interesting,\n",
      "that he's not a native English speaker, and here, that gave him an edge in this machine translation that led to attention and then led to transformer.\n",
      "So that's really fascinating. \"I expressed a soft search a softmax and then weighted averaging of the [INAUDIBLE] states.\n",
      "And basically, to my great excitement, this worked from the very first try.\" So really, I think, interesting piece of history.\n",
      "And as it later turned out that the name of RNN search was kind of lame, so the better name attention came\n",
      "from Yoshua on one of the final passes as they went over the paper. So maybe Attention is All You Need\n",
      "would have been called RNN Search is All You Need, but we have Yoshua Bengio to thank for a little bit of better name, I would say.\n",
      "So apparently, that's the history of this, which I thought was interesting. OK, so that brings us to 2017, which is Attention\n",
      "is All You Need. So this attention component, which in Dzmitry's paper was just one small segment,\n",
      "and there's all this bidirectional RNN, RNN and decoder, and this Attention All You Need paper is saying,\n",
      "OK, you can actually delete everything. What's making this work very well is just attention by itself. And so delete everything, keep attention.\n",
      "And then what's remarkable about this paper actually is usually, you see papers that are very incremental. They add one thing, and they show that it's better.\n",
      "But I feel like Attention is All You Need was like a mix of multiple things at the same time. They were combined in a very unique way,\n",
      "and then also achieve a very good local minimum in the architecture space. And so to me, this is really a landmark paper\n",
      "that is quite remarkable and, I think, had quite a lot of work behind the scenes.\n",
      "So delete all the RNN, just keep attention. Because attention operates over sets-- and I'm going to go to this in a second--\n",
      "you now need to positionally encode your inputs because attention doesn't have the notion of space by itself.\n",
      "I have to be very careful. They adopted this residual network structure\n",
      "from resonance. They interspersed attention with multi-layer perceptrons. They used layer norms, which came from a different paper.\n",
      "They introduced the concept of multiple heads of attention that were applied in parallel. And they gave us, I think, like a fairly good set\n",
      "of hyperparameters that to this day are used. So the expansion factor in the multi-layer perceptron goes up\n",
      "by 4X-- and we'll go into a bit more detail-- and this 4X has stuck around. And I believe there's a number of papers\n",
      "that try to play with all kinds of little details of the transformer, and nothing sticks because this is actually\n",
      "quite good. The only thing to my knowledge that didn't stick was this reshuffling of the layer norms\n",
      "to go into the prenorm version where here you see the layer norms are after the multiheaded attention feed\n",
      "forward. They just put them before instead. So just reshuffling of layer norms, but otherwise, the TPTs and everything else that you're seeing today\n",
      "is basically the 2017 architecture from 5 years ago. And even though everyone is working on it,\n",
      "it's been proven remarkably resilient, which I think is real interesting. There are innovations that, I think,\n",
      "have been adopted also in positional encoding. It's more common to use different rotary and relative\n",
      "positional encoding and so on. So I think there have been changes, but for the most part, it's proven very resilient.\n",
      "So really quite an interesting paper. Now, I wanted to go into the attention mechanism.\n",
      "And I think, the way I interpret it is not similar to the ways\n",
      "that I've seen it presented before. So let me try a different way of how I see it. Basically, to me, attention is kind of like the communication\n",
      "phase of the transformer, and the transformer interweaves two phases of the communication phase, which\n",
      "is the multi-headed attention, and the computation stage, which is this multilayered perceptron or [INAUDIBLE].\n",
      "So in the communication phase, it's really just a data dependent message passing on directed graphs.\n",
      "And you can think of it as OK, forget everything with machine translation, everything. Let's just-- we have directed graphs.\n",
      "At each node, you are storing a vector. And then let me talk now about the communication\n",
      "phase of how these vectors talk to each other and this directed graph. And then the compute phase later is just a multi-perceptron, which then basically acts on every node\n",
      "individually. But how do these nodes talk to each other in this directed graph?\n",
      "So I wrote like some simple Python-- I wrote this in Python basically to create\n",
      "one round of communication of using attention as the message passing scheme.\n",
      "So here, a node has this private data vector, as you can think of it as private information\n",
      "to this node. And then it can also emit a key, a query, and a value. And simply, that's done by linear transformation\n",
      "from this node. So the key is what are the things that I am--\n",
      "sorry. The query is what are the things that I'm looking for? The key is what other the things that I have? And the value is what are the things that I will communicate?\n",
      "And so then when you have your graph that's made up of nodes in some random edges, when you actually have these nodes communicating, what's happening is\n",
      "you loop over all the nodes individually in some random order, and you're at some node,\n",
      "and you get the query vector q, which is, I'm a node in some graph, and this\n",
      "is what I'm looking for. And so that's just achieved via this linear transformation here. And then we look at all the inputs that point to this node,\n",
      "and then they broadcast what are the things that I have, which is their keys. So they broadcast the keys.\n",
      "I have the query, then those interact by dot product to get scores.\n",
      "So basically, simply by doing dot product, you get some unnormalized weighting of the interestingness of all of the information in the nodes\n",
      "that point to me and to the things I'm looking for. And then when you normalize that with softmax, so it just sums to 1, you basically just\n",
      "end up using those scores, which now sum to 1 in our probability distribution, and you do a weighted sum of the values\n",
      "to get your update. So I have a query. They have keys, dot products to get interestingness or like\n",
      "affinity, softmax to normalize it, and then weighted sum of those values flow to me and update me.\n",
      "And this is happening for each node individually. And then we update at the end. And so this kind of a message passing scheme\n",
      "is at the heart of the transformer. And it happens in the more vectorized batched way\n",
      "that is more confusing and is also interspersed with layer norms and things like that to make the training behave\n",
      "better. But that's roughly what's happening in the attention mechanism, I think, on a high level.\n",
      "So yeah, so in the communication phase of the transformer, then\n",
      "this message passing scheme happens in every head in parallel and then in every layer in series\n",
      "and with different weights each time. And that's it as far as the multi-headed attention goes.\n",
      "And so if you look at these encooder-decoder models, you can think of it then in terms of the connectivity of these nodes in the graph.\n",
      "You can think of it as like, OK, all these tokens that are in the encoder that we want to condition on, they are fully connected to each other.\n",
      "So when they communicate, they communicate fully when you calculate their features. But in the decoder, because we are\n",
      "trying to have a language model, we don't want to have communication for future tokens because they give away the answer at this step.\n",
      "So the tokens in the decoder are fully connected from all the encoder states, and then they\n",
      "are also fully connected from everything that is decoding. And so you end up with this triangular structure\n",
      "in the data graph. But that's the message passing scheme that this basically implements.\n",
      "And then you have to be also a little bit careful because in the cross attention here with the decoder, you consume the features from the top of the encoder.\n",
      "So think of it as in the encoder, all the nodes are looking at each other, all the tokens are looking at each other many, many times.\n",
      "And they really figure out what's in there, and then the decoder when it's looking only at the top nodes.\n",
      "So that's roughly the message passing scheme. I was going to go into more of an implementation of a transformer. I don't know if there's any questions about this.\n",
      "[INAUDIBLE] self-attention and multi-headed attention, but what is the advantage of [INAUDIBLE]??\n",
      "Yeah, so self-attention and multi-headed attention, so the multi-headed attention is just this attention scheme,\n",
      "but it's just applied multiple times in parallel. Multiple heads just means independent applications of the same attention.\n",
      "So this message passing scheme basically just happens in parallel multiple times with different weights for the query, key, and value.\n",
      "So you can almost look at it like in parallel, I'm looking for, I'm seeking different kinds of information from different nodes.\n",
      "And I'm collecting it all in the same node. It's all done in parallel. So heads is really just copy-paste in parallel.\n",
      "And layers are copy-paste but in series.\n",
      "Maybe that makes sense. And self-attention, when it's self-attention,\n",
      "what it's referring to is that the node here produces each node here. So as I described it here, this is really self-attention\n",
      "because every one of these nodes produces a key query and a value from this individual node. When you have cross-attention, you have one cross-attention\n",
      "here, coming from the encoder. That just means that the queries are still produced from this node, but the keys and the values\n",
      "are produced as a function of nodes that are coming from the encoder.\n",
      "So I have my queries because I'm trying to decode some-- the fifth word in the sequence.\n",
      "And I'm looking for certain things because I'm the fifth word. And then the keys and the values in terms of the source of information that could answer my queries\n",
      "can come from the previous nodes in the current decoding sequence or from the top of the encoder.\n",
      "So all the nodes that have already seen all of the encoding tokens many, many times cannot broadcast\n",
      "what they contain in terms of information. So I guess, to summarize, the self-attention is--\n",
      "sorry, cross-attention and self-attention only differ in where the piece and the values come from.\n",
      "Either the keys and values are produced from this node, or they are produced from some external source like an encoder\n",
      "and the nodes over there. But algorithmically, is the same mathematical operations.\n",
      "Question. Yeah, OK. So two questions for you. First question is, in the message passing [INAUDIBLE]\n",
      "So think of-- so each one of these nodes is a token.\n",
      "I guess they don't have a very good picture of it in the transformer. But this node here could represent the third word\n",
      "in the output in the decoder, and in the beginning, it is just the embedding of the word.\n",
      "And then, OK, I have to think through this analogy a little bit more. I came up with it this morning.\n",
      "[LAUGHTER] [INAUDIBLE]\n",
      "What example of instantiation [INAUDIBLE] nodes\n",
      "as in in blocks were embedding? These nodes are basically the vectors.\n",
      "I'll go to an implementation. I'll go to the implementation, and then maybe I'll make the connections to the graph.\n",
      "So let me try to first go to-- let me now go to, with this intuition in mind, at least, to a nanoGPT, which is a concrete implementation\n",
      "of a transformer that is very minimal. So I worked on this over the last few days, and here it is reproducing GPT-2 on open web text.\n",
      "So it's a pretty serious implementation that reproduces GPT-2, I would say, and provide it enough compute--\n",
      "This was one node of 8 GPUs for 38 hours or something like that, if I remember correctly. And it's very readable.\n",
      "It's 300 lines, so everyone can take a look at it. And yeah, let me basically briefly step through it.\n",
      "So let's try to have a decoder-only transformer. So what that means is that it's a language model.\n",
      "It tries to model the next word in the sequence or the next character in the sequence.\n",
      "So the data that we train on this is always some kind of text. So here's some fake Shakespeare. Sorry, this is real Shakespeare.\n",
      "We're going to produce fake Shakespeare. So this is called a Tiny Shakespeare dataset, which is one of my favorite toy datasets.\n",
      "You take all of Shakespeare, concatenate it, and it's 1 megabyte file, and then you can train language models on it and get infinite Shakespeare, if you like,\n",
      "which I think is kind of cool. So we have a text. The first thing we need to do is we need to convert it to a sequence of integers\n",
      "because transformers natively process-- you can't plug text into transformer.\n",
      "You need to somehow encode it. So the way that encoding is done is we convert, for example, in the simplest case, every character gets an integer, and then instead of \"hi\n",
      "there,\" we would have this sequence of integers. So then you can encode every single character as an integer\n",
      "and get a massive sequence of integers. You just concatenate it all into one large, long one-dimensional sequence.\n",
      "And then you can train on it. Now, here, we only have a single document. In some cases, if you have multiple independent documents,\n",
      "what people like to do is create special tokens, and they intersperse those documents with those special end of text tokens\n",
      "that they splice in between to create boundaries. But those boundaries actually don't have any modeling impact.\n",
      "It's just that the transformer is supposed to learn via backpropagation that the end of document sequence means that you should wipe the memory.\n",
      "OK, so then we produce batches. So these batches of data just mean that we go back to the one-dimensional sequence,\n",
      "and we take out chunks of this sequence. So say, if the block size is 8, Then the block size indicates\n",
      "the maximum length of context that your transformer will process. So if our block size is 8, that means\n",
      "that we are going to have up to eight characters of context to predict the ninth character in a sequence.\n",
      "And the batch size indicates how many sequences in parallel we're going to process. And we want this to be as large as possible,\n",
      "so we're fully taking advantage of the GPU and the parallels [INAUDIBLE] So in this example, we're doing a 4 by 8 batches.\n",
      "So every row here is independent example and then every row here is a small chunk of the sequence\n",
      "that we're going to train on. And then we have both the inputs and the targets at every single point here.\n",
      "So to fully spell out what's contained in a single 4 by 8 batch to the transformer-- I sort of compact it here--\n",
      "so when the input is 47, by itself, the target is 58.\n",
      "And when the input is the sequence 47, 58, the target is one. And when it's 47, 58, 1, the target is 51 and so on.\n",
      "So actually, the single batch of examples that score by 8 actually has a ton of individual examples that we are expecting a transformer\n",
      "to learn on in parallel. And so you'll see that the batches are learned on completely independently, but the time dimension here along\n",
      "horizontally is also trained on in parallel. So your real batch size is more like B times T.\n",
      "And it's just that the context grows linearly for the predictions that you make along the T direction\n",
      "in the model. So this is all the examples that the model will learn from, this single batch.\n",
      "So now, this is the GPT class. And because this is a decoder-only model,\n",
      "so we're not going to have an encoder because there's no English we're translating from-- we're not trying to condition in some other external\n",
      "information. We're just trying to produce a sequence of words that follow each other or likely to.\n",
      "So this is all PyTorch, and I'm going slightly faster because I'm assuming people have taken 231 or something along those lines.\n",
      "But here in the forward pass, we take these indices, and then we both encode the identity of the indices,\n",
      "just via an embedding lookup table. So every single integer, we index into a lookup table of\n",
      "vectors in this, and end up embedding, and pull out the word vector for that token.\n",
      "And then because the transformer by itself doesn't actually-- the process is set natively.\n",
      "So we need to also positionally encode these vectors so that we basically have both the information about the token identity and its place in the sequence from 1\n",
      "to block size. Now, the information about what and where is combined additively, so the token embeddings\n",
      "and the positional embeddings are just added exactly as here. So then there's optional dropout,\n",
      "this x here basically just contains the set of words and their positions,\n",
      "and that feeds into the blocks of transformer. And we're going to look into what's block here. But for here, for now, this is just a series\n",
      "of blocks in a transformer. And then in the end, there's a layer norm, and then you're decoding the logits\n",
      "for the next word or next integer in a sequence, using the linear projection of the output of this transformer\n",
      "So LM head here, a short core language model head. It's just a linear function.\n",
      "So basically, positionally encode all the words, feed them into a sequence of blocks,\n",
      "and then apply a linear layer to get the probability distribution for the next character.\n",
      "And then if we have the targets, which we produced in the data order-- and you'll notice that the targets are just\n",
      "the inputs offset by one in time-- then those targets feed into a cross entropy loss.\n",
      "So this is just a negative log likelihood typical classification loss. So now let's drill into what's here in the blocks.\n",
      "So these blocks that are applied sequentially, there's, again, as I mentioned, this communicate phase and the compute phase.\n",
      "So in the communicate phase, all the nodes get to talk to each other, and so these nodes are basically,\n",
      "if our block size is 8, then we are going to have eight nodes in this graph.\n",
      "There's eight nodes in this graph. The first node is pointed to only by itself. The second node is pointed to by the first node and itself.\n",
      "The third node is pointed to by the first two nodes and itself, et cetera. So there's eight nodes here.\n",
      "So you apply-- there's a residual pathway and x. You take it out. You apply a layer norm, and then the self-attention\n",
      "so that these communicate, these eight nodes communicate. But you have to keep in mind that the batch is 4. So because batch is 4, this is also applied--\n",
      "so we have eight nodes communicating, but there's a batch of four of them individually communicating in one of those eight nodes.\n",
      "There's no crisscross across the batch dimension, of course. There's no batch anywhere luckily. And then once they've changed information,\n",
      "they are processed using the multi-layer perceptron. And that's the compute phase.\n",
      "And then also here we are missing the cross-attention\n",
      "because this is a decoder-only model. So all we have is this step here, the multi-headed attention, and that's this line, the communicate phase.\n",
      "And then we have the feed forward, which is the MLP, and that's the compute phase.\n",
      "I'll take question's a bit later. Then the MLP here is fairly straightforward.\n",
      "The MLP is just individual processing on each node, just transforming the feature representation at that node.\n",
      "So applying a two-layer neural net with a GELU nonlinearity, which is just\n",
      "think of it as a ReLU or something like that. It's just a nonlinearity. And then MLP is straightforward.\n",
      "I don't think there's anything too crazy there. And then this is the causal self-attention part, the communication phase.\n",
      "So this is like the meat of things and the most complicated part. It's only complicated because of the batching\n",
      "and the implementation detail of how you mask the connectivity in the graph so that you can't obtain\n",
      "any information from the future when you're predicting your token. Otherwise, it gives away the information. So if I'm the fifth token and if I'm the fifth position,\n",
      "then I'm getting the fourth token coming into the input, and I'm attending to the third, second, and first,\n",
      "and I'm trying to figure out what is the next token. Well then, in this batch, in the next element\n",
      "over in the time dimension, the answer is at the input. So I can't get any information from there.\n",
      "So that's why this is all tricky, but basically, in the forward pass, we are calculating the queries, keys, and values based on x.\n",
      "So these are the keys, queries, and values. Here, when I'm computing the attention, I have the queries matrix multiplying the piece.\n",
      "So this is the dot product in parallel for all the queries and all the keys in all the heads.\n",
      "So I failed to mention that there's also the aspect of the heads, which is also done all in parallel\n",
      "here. So we have the batch dimension, the time dimension, and the head dimension, and you end up with five-dimensional tensors, and it's all really confusing.\n",
      "So I invite you to step through it later and convince yourself that this is actually doing the right thing. But basically, you have the batch dimension, the head\n",
      "dimension and the time dimension, and then you have features at them. And so this is evaluating for all the batch elements, for all\n",
      "the head elements, and all the time elements, the simple Python that I gave you earlier, which is query\n",
      "dot product p. Then here, we do a masked_fill, and what this is doing is it's basically clamping the attention between the nodes\n",
      "that are not supposed to communicate to be negative infinity. And we're doing negative infinity because we're about to softmax, and so negative infinity will\n",
      "make basically the attention that those elements be zero. And so here we are going to basically end up\n",
      "with the weights, the affinities between these nodes, optional\n",
      "dropout. And then here, attention matrix multiply v is basically\n",
      "the gathering of the information according to the affinities we calculated. And this is just a weighted sum of the values\n",
      "at all those nodes. So this matrix multiplies is doing that weighted sum. And then transpose contiguous view\n",
      "because it's all complicated and batched in five-dimensional tensors, but it's really not doing anything, optional drop out,\n",
      "and then a linear projection back to the residual pathway. So this is implementing the communication phase here.\n",
      "Then you can train this transformer. And then you can generate infinite Shakespeare.\n",
      "And you will simply do this by-- because our block size is 8, we start with a sum token,\n",
      "say like, I used in this case, you can use something like a new line as the start token.\n",
      "And then you communicate only to yourself because there's a single node, and you get the probability distribution for the first word\n",
      "in the sequence. And then you decode it for the first character in the sequence. You decode the character.\n",
      "And then you bring back the character, and you re-encode it as an integer. And now, you have the second thing.\n",
      "And so you get-- OK, we're at the first position, and this is whatever integer it is, add the positional encodings,\n",
      "goes into the sequence, goes in the transformer, and again, this token now communicates with the first token and it's identity.\n",
      "And so you just keep plugging it back. And once you run out of the block size, which is eight, you start to crawl, because you can never\n",
      "have watt size more than eight in the way you've trained this transformer. So we have more and more context until eight. And then if you want to generate beyond eight,\n",
      "you have to start cropping because the transformer only works for eight elements in time dimension. And so all of these transformers in the [INAUDIBLE] setting\n",
      "have a finite block size or context length, and in typical models, this will be 1,024 tokens or 2,048\n",
      "tokens, something like that. But these tokens are usually like BPE tokens, or SentencePiece tokens, or WorkPiece tokens.\n",
      "There's many different encodings. So it's not like that long. And so that's why, I think, [INAUDIBLE].. We really want to expand the context size,\n",
      "and it gets gnarly because the attention is sporadic in the [INAUDIBLE] case. Now, if you want to implement an encoder instead of a decoder\n",
      "attention. Then all you have to do is this [INAUDIBLE] and you just delete that line.\n",
      "So if you don't mask the attention, then all the nodes communicate to each other, and everything is allowed, and information\n",
      "flows between all the nodes. So if you want to have the encoder here, just delete.\n",
      "All the encoder blocks will use attention where this line is deleted. That's it. So you're allowing whatever-- this encoder might store say,\n",
      "10 tokens, 10 nodes, and they are all allowed to communicate to each other going up the transformer.\n",
      "And then if you want to implement cross-attention, so you have a full encoder-decoder transformer, not just a decoder-only transformer or a GPT.\n",
      "Then we need to also add cross-attention in the middle. So here, there is a self-attention piece where all\n",
      "the-- there's a self-attention piece, a cross-attention piece, and this MLP. And in the cross-attention, we need\n",
      "to take the features from the top of the encoder. We need to add one more line here, and this would be the cross-attention instead of a--\n",
      "I should have implemented it instead of just pointing, I think. But there will be a cross-attention line here.\n",
      "So we'll have three lines because we need to add another block. And the queries will come from x but the keys\n",
      "and the values will come from the top of the encoder. And there will be basic code information\n",
      "flowing from the encoder, strictly to all the nodes inside x. And then that's it.\n",
      "So it's a very simple modifications on the decoder attention. So you'll hear people talk that you have\n",
      "a decoder-only model like GPT. You can have an encoder-only model like BERT, or you can have an encoder-decoder model\n",
      "like say T5, doing things like machine translation. And in BERT, you can't train it using this language modeling\n",
      "setup that's utter aggressive, and you're just trying to predict next [INAUDIBLE] in the sequence. You're training it doing slightly different objectives.\n",
      "You're putting in the full sentence, and, the full sentence is allowed to communicate fully. And then you're trying to classify sentiment or something\n",
      "like that. So you're not trying to model the next token in the sequence. So these are trained slightly different\n",
      "using masking and other denoising techniques.\n",
      "OK. So that's like the transformer. I'm going to continue. So yeah, maybe more questions.\n",
      "[INAUDIBLE]\n",
      "This is like we are enforcing these constraints on it by just masking [INAUDIBLE]\n",
      "So I'm not sure if I fully follow. So there's different ways to look at this analogy, but one analogy is you can interpret\n",
      "this graph as really fixed. It's just that every time we do the communicate, we are using different weights.\n",
      "You can look at it that way. So if we have block size of eight in my example, we would have eight nodes. Here we have 2, 4, 6.\n",
      "OK, so we'd have eight nodes. They would be connected in-- you lay them out, and you only connect from left to right.\n",
      "[INAUDIBLE]\n",
      "Why would they connect-- usually, the connections don't change as a function of the data or something like that-- [INAUDIBLE]\n",
      "I don't think I've seen a single example where the connectivity changes dynamically in the function data. Usually, the connectivity is fixed.\n",
      "If you have an encoder, and you're training a BERT, you have how many tokens you want, and they are fully connected.\n",
      "And if you have a decoder-only model, you have this triangular thing, and if you have encoder-decoder, then you have\n",
      "awkwardly two pools of nodes. Yeah.\n",
      "Go ahead. [INAUDIBLE] I wonder, you know much more about this\n",
      "than I know. But do you have a sense of like if you ran [INAUDIBLE]\n",
      "In my head, I'm thinking [INAUDIBLE] but then you also\n",
      "have different things for one or more of [INAUDIBLE]---- Yeah, it's really hard to say, so that's\n",
      "why I think this paper is so interesting because like, yeah, usually, you'd see like the path, and maybe they had path internally. They just didn't publish it.\n",
      "All you can see is things that didn't look like a transformer. I mean, you have ResNets, which have lots of this.\n",
      "But a ResNet would be like this, but there's no self-attention component. But the MLP is there kind of in a ResNet.\n",
      "So a ResNet looks very much like this except there's no-- you can use layer norms in ResNets, I believe, as well.\n",
      "Typically, sometimes, they can be batch norms. So it is kind of like a ResNet. It is like they took a ResNet, and they\n",
      "put in a self-attention block in addition to the preexisting MLP block, which is kind of like convolutions.\n",
      "And MLP was strictly speaking deconvolution, one by one convolution, but I think\n",
      "the idea is similar in that MLP is just like a typical weights,\n",
      "nonlinearity weights operation.\n",
      "But I will say, yeah, this is kind of interesting because a lot of work is not there, and then they give you this transformer.\n",
      "And then it turns out 5 years later, it's not changed, even though everyone's trying to change it. So it's interesting to me that it's like a package,\n",
      "in like a package, which I think is really interesting historically. And I also talked to paper authors,\n",
      "and they were unaware of the impact that the transformer would have at the time. So when you read this paper, actually, it's unfortunate\n",
      "because this is the paper that changed everything, but when people read it, it's like question marks because it reads like a pretty random machine translation\n",
      "paper. It's like, oh, we're doing machine translation. Oh, here's a cool architecture. OK, great, good results.\n",
      "It doesn't know what's going to happen. [LAUGHS] And so when people read it today, I think they're confused potentially.\n",
      "I will have some tweets at the end, but I think I would have renamed it with the benefit of hindsight of like, well, I'll get to it.\n",
      "[INAUDIBLE]\n",
      "Yeah, I think that's a good question as well. Currently, I mean, I certainly don't love the autoregressive modeling approach.\n",
      "I think it's kind of weird to sample a token and then commit to it. So maybe there are some ways, some hybrids\n",
      "with the Fusion as an example, which I think would be really cool, or we'll find some other ways to edit the sequences later but still\n",
      "in our regressive framework. But I think the Fusion is like an up and coming modeling\n",
      "approach that I personally find much more appealing. When I sample text, I don't go chunk, chunk, chunk, and commit.\n",
      "I do a draft one, and then I do a better draft two. And that feels like a diffusion process.\n",
      "So that would be my hope. OK, also a question.\n",
      "So yeah, you'd think the [INAUDIBLE]\n",
      "And then once we have the edge rates, we just have to multiply it by the values, and then you just [INAUDIBLE] it. Yes, yeah, it's right.\n",
      "And you think there's MLG within graph neural networks and they'll potentially--\n",
      "I find the graph neural networks like a confusing term because, I mean, yeah, previously,\n",
      "there, was this notion of-- I feel like maybe today everything is a graph neural network because a transformer is a graph neural network\n",
      "processor. The native representation that the transformer operates over is sets that are connected by edges in a direct way.\n",
      "And so that's the native representation, and then, yeah. OK, I should go on because I still have 30 slides.\n",
      "[INAUDIBLE]\n",
      "Oh yeah, yeah, the root DE, I think, it basically like if you're initializing with random weights\n",
      "setup from a [INAUDIBLE] as your dimension size grows, so does your values, the variance grows.\n",
      "And then your softmax will just become the one half vector. So it's just a way to control the variance\n",
      "and bring it to always be in a good range for softmax and nice diffused distribution.\n",
      "OK, so it's almost like an initialization thing.\n",
      "OK, so transformers have been applied to all the other fields, and the way this was done\n",
      "is in my opinion, ridiculous ways honestly because I was a computer vision person,\n",
      "and you have ComNets, and they make sense. So what we're doing now with VITs as an example is you take an image and you chop it up into little squares.\n",
      "And then those squares, literally, feed into a transformer, and that's it, which is kind of ridiculous.\n",
      "And so, I mean, yeah, and so the transformer doesn't even, in the simplest case, really know where\n",
      "these patches might come from. They are usually positionally encoded, but it has to rediscover a lot of the structure,\n",
      "I think, of them in some ways. And it's kind of weird to approach it that way.\n",
      "But it's just the simplest baseline of just chomping up big images into small squares and feeding them in as the individual nodes actually\n",
      "works fairly well. And then this is in a transformer encoder, so all the patches are talking to each other throughout the entire transformer.\n",
      "And the number of nodes here would be like nine.\n",
      "Also, in speech recognition, you just take your melSpectrogram, and you chop it up into slices and you feed them into a transformer.\n",
      "So there was paper like this, but also Whisper. Whisper is a copy-paste transformer. If you saw Whisper from OpenAI, you just chop up melSpectrogram\n",
      "and feed it into a transformer and then pretend you're dealing with text. And it works very well.\n",
      "Decision transformer in RL, you take your states, actions, and reward that you experience in environment, and you just pretend it's a language.\n",
      "Then you start to model the sequences of that, and then you can use that for planning later. That works really well.\n",
      "Even things AlphaFold, so we were briefly talking about molecules and how you can plug them in. So at the heart of AlphaFold, computationally,\n",
      "is also a transformer. One thing I wanted to also say about transformers is I find that they're very flexible,\n",
      "and I really enjoy that. I'll give you an example from Tesla. You have a ComNet that takes an image\n",
      "and makes predictions about the image. And then the big question is, how do you feed in extra information? And it's not always trivial like say, I\n",
      "had additional information that I want to inform that I want the outputs to be informed by. Maybe I have other sensors like Radar.\n",
      "Maybe I have some map information, or a vehicle type, or some audio. And the question is, how do you feed information into a ComNet?\n",
      "Like where do you feed it in? Do you concatenate it? Do you add it? At what stage?\n",
      "And so with a transformer, it's much easier because you just take whatever you want, you chop it up into pieces, and you feed it in with a set\n",
      "of what you had before. And you let the self-attention figure out how everything should communicate. And that actually apparently works.\n",
      "So just chop up everything and throw it into the mix is like the way. And it frees neural nets from this burgeon\n",
      "of Euclidean space, where previously you had to arrange your computation to conform to the Euclidean\n",
      "space or three dimensions of how you're laying out the compute. Like the compute actually kind of happens in almost like 3D space if you think about it.\n",
      "But in attention, everything is just sets. So it's a very flexible framework, and you can just throw in stuff into your conditioning set.\n",
      "And everything just self-attended over. So it's quite beautiful from that perspective. OK, so now what exactly makes transformers so effective?\n",
      "I think a good example of this comes from the GPT-3 paper, which I encourage people to read.\n",
      "Language Models of Few-Shot Learners. I would have probably renamed this a little bit. I would have said something like transformers\n",
      "are capable of in-context learning or meta-learning. That's like what makes them really special.\n",
      "So basically the setting that they're working with is, OK, I have some context, and I'm trying-- like say, a passage. This is just one example of many.\n",
      "I have a passage, and I'm asking questions about it. And then as part of the context in the prompt,\n",
      "I'm giving the questions and the answers. So I'm giving one example of question-answer, another example of question-answer, another example of question-answer, and so on.\n",
      "And this becomes-- Oh yeah, people are going to have to leave soon, huh? OK, is this really important?\n",
      "Let me think. OK, so what's really interesting is basically\n",
      "like with more examples given in a context, the accuracy improves.\n",
      "And so what that can set is that the transformer is able to somehow learn in the activations without doing any gradient descent\n",
      "in a typical fine-tuning fashion. So if you fine-tune, you have to give an example and the answer, and you fine-tune it, using gradient descent.\n",
      "But it looks like the transformer internally in its weights is doing something that looks like potentially gradient, some kind of a metalearning in the weights of the transformer\n",
      "as it is reading the prompt. And so in this paper, they go into, OK, distinguishing this outer loop with stochastic gradient\n",
      "descent in this inner loop of the intercontext learning. So the inner loop is as the transformer is reading the sequence almost and the outer loop is the training\n",
      "by gradient descent. So basically, there's some training happening in the activations of the transformer as it is consuming a sequence that\n",
      "may be very much looks like gradient descent. And so there are some recent papers that hint at this and study it.\n",
      "And so as an example, in this paper here, they propose something called the draw operator. And they argue that the raw operator is implemented\n",
      "by transformer, and then they show that you can implement things like ridge regression on top of the raw operator. And so this is giving--\n",
      "There are papers hinting that maybe there is some thing that looks like gradient-based learning inside the activations of the transformer.\n",
      "And I think this is not impossible to think through because what is gradient-based learning? Overpass, backward pass, and then update.\n",
      "Oh, that looks like a ResNet, right, because you're adding to the weights. So the start of initial random set of weights,\n",
      "forward pass, backward pass, and update your weights, and then forward pass, backward pass, update the weights. Looks like a ResNet.\n",
      "Transformer is a ResNet, so much more hand-wavey,\n",
      "but basically, some papers are trying to hint at why that would be potentially possible. And then I have a bunch of tweets I just copy-pasted here\n",
      "in the end. This was like meant for general consumption, so they're a bit more high-level and hypey a little bit.\n",
      "But I'm talking about why this architecture is so interesting and why potentially it became so popular.\n",
      "And I think it simultaneously optimizes three properties that, I think, are very desirable. Number one, the transformer is very\n",
      "expressive in the forward pass. It sort of like it's able to implement very interesting functions, potentially functions\n",
      "that can even do meta-learning. Number two, it is very optimizable thanks to things like residual connections, layer nodes,\n",
      "and so on. And number three, it's extremely efficient. This is not always appreciated, but the transformer, if you look at the computational graph,\n",
      "is a shallow, wide network, which is perfect to take advantage of the parallelism of GPUs. So I think the transformer was designed very deliberately\n",
      "to run efficiently on GPUs. There's previous work like neural GPU that I really enjoy as well, which is really just\n",
      "like how do we design neural nets that are efficient on GPUs and thinking backwards from the constraints of the hardware, which I think is a very interesting way\n",
      "to think about it.\n",
      "Oh yeah, so here, I'm saying, I probably would have called-- I probably would've called the transformer a general purpose\n",
      "efficient optimizable computer instead of attention is all you need. That's what I would have maybe in hindsight called that paper.\n",
      "It's proposing a model that is very general purpose, so\n",
      "forward passes, expressive. It's very efficient in terms of GPU usage and is easily optimizable by gradient descent and trains\n",
      "very nicely. And then I have some other hype tweets here.\n",
      "Anyway, so you can read them later. But I think this one is maybe interesting. So if previous neural nets are special purpose computers\n",
      "designed for a specific task, GPT is a general purpose computer, reconfigurable at runtime\n",
      "to run natural language programs. So the programs are given as prompts,\n",
      "and then GPT runs the program by completing the document. So I really like these analogies personally to computer.\n",
      "It's just like a powerful computer, and it's optimizable by gradient descent.\n",
      "And I don't know--\n",
      "OK, yeah. That's it. [LAUGHTER] You can read the tweets later, but that's for now. I'll just thank you.\n",
      "I'll just leave this up.\n",
      "Sorry, I just found this tweet. So turns out that if you scale up the training set and use a powerful enough neural net like a transformer,\n",
      "the network becomes a kind of general purpose computer over text. So I think that's nice way to look at it. And instead of performing a single text sequence,\n",
      "you can design the sequence in the prompt. And because the transformer is both powerful but also is trained on large enough, very hard data set,\n",
      "it becomes this general purpose text computer. And so I think that's kind of interesting way to look at it.\n",
      "Yeah. [INAUDIBLE]\n",
      "And I guess my question is [INAUDIBLE] how much do you think [INAUDIBLE]?\n",
      "really because it's mostly more efficient or [INAUDIBLE]\n",
      "So I think there's a bit of that. Yeah, so I would say RNNs in principle, yes, they can implement arbitrary programs.\n",
      "I think, it's like a useless statement to some extent because they're probably-- I'm not sure that they're probably expressive\n",
      "because in a sense of power and that they can implement these arbitrary functions.\n",
      "But they're not optimizable. And they're certainly not efficient because they are serial computing devices.\n",
      "So if you look at it as a compute graph, RNNs are very long, thin compute graph.\n",
      "What if you stretched out the neurons and you looked-- like take all the individual neurons interconnectivity, and stretch them out, and try to visualize them.\n",
      "RNNs would be like a very long graph and that's bad. And it's bad also for optimizability because I don't exactly know why,\n",
      "but just the rough intuition is when you're backpropagating, you don't want to make too many steps. And so transformers are a shallow wide graph, and so\n",
      "from supervision to inputs is a very small number of hops. And it's a long residual pathways,\n",
      "which make gradients flow very easily. And there's all these layer norms to control the scales of all of those activations.\n",
      "And so there's not too many hops, and you're going from supervision to input very quickly and just flows through the graph.\n",
      "And it can all be done in parallel, so you don't need to do this-- encoder and decoder RNNs, you have to go from first word,\n",
      "then second word, then third word. But here in transformer, every single word was processed completely in parallel, which is kind of a--\n",
      "So I think all of these are really important because all of these are really important. And I think number 3 is less talked about but extremely\n",
      "important because in deep learning scale matters. And so the size of the network that you can train it\n",
      "gives you is extremely important. And so if it's efficient on the current hardware, then you can make it bigger.\n",
      "You mentioned that if you do it with multiple modalities of data, [INAUDIBLE].\n",
      "How does that actually work? Do you leave the different data as different token, or is it [INAUDIBLE]?\n",
      "No, so yeah, so you take your image, and you apparently chop them up into patches. So there's the first thousand tokens or whatever.\n",
      "And now, I have a special-- so radar could be also, but I don't actually\n",
      "want to make a representation of radar. But you just need to chop it up and enter it.\n",
      "And then you have to encode it somehow. Like the transformer needs to know that they're coming from radar. So you create a special--\n",
      "you have some kind of a special token of that to-- these radar tokens are what's slightly different in the representation, and it's\n",
      "learnable by gradient descent. And like vehicle information would also come in with a special embedded token that can be learned.\n",
      "So-- So how do you line those before really-- Actually, but you don't. It's all just a set.\n",
      "And there's-- Even the [INAUDIBLE] Yeah, it's all just a set, but you can positionally\n",
      "encode these sets if you want. So positional encoding means you can\n",
      "hardwire, for example, the coordinates like using [INAUDIBLE]. You can hardwire that, but it's better\n",
      "if you don't hardwire the position. It's just a vector that is always hanging out the dislocation. Whatever content is there, it just adds on it.\n",
      "And this vector is trainable by background. That's how you do it.\n",
      "Good point. I don't really like the [INAUDIBLE]..\n",
      "They seem to work, but it seems like they're sometimes [INAUDIBLE]\n",
      "I'm not sure if I understand your question. [LAUGHTER] So I mean the positional encoders like they're actually like not--\n",
      "OK, so they have very little inductive bias or something like that. They're just vectors hanging out in location always,\n",
      "and you're trying to help the network in some way. And I think the intuition is good,\n",
      "but if you have enough data, usually, trying to mess with it is a bad thing. Trying to enter knowledge when you\n",
      "have enough knowledge in the data set itself is not usually productive. So it all really depends on what scale you want. If you have infinity data, then you actually\n",
      "want to encode less and less. That turns out to work better. And if you have very little data, then actually, you do want to encode some biases.\n",
      "And maybe if you have a much smaller data set, then maybe convolutions are a good idea because you actually have this bias coming from your filters.\n",
      "But I think-- so the transformer is extremely general, but there are ways to mess with the encodings\n",
      "to put in more structure. Like you could, for example, encode [INAUDIBLE] and fix it, or you could actually go to the attention mechanism\n",
      "and say, OK, if my image is chopped up into patches, this patch can only communicate to this neighborhood.\n",
      "And you just do that in the attention matrix, you just mask out whatever you don't want to communicate.\n",
      "And so people really play with this because the full attention is inefficient. So they will intersperse, for example, layers\n",
      "that only communicate in little patches and then layers that communicate globally. And they will do all kinds of tricks like that.\n",
      "So you can slowly bring in more inductive bias. You would do it, but the inductive biases are like they're factored out from the core transformer.\n",
      "And they are factored out, and the interconnectivity of the nodes. And they are factored out in the positionally--\n",
      "and you can mess with this for computation. [INAUDIBLE]\n",
      "So there's probably about 200 papers on this now if not more. They're kind of hard to keep track of.\n",
      "Honestly, like my Safari browser, which is-- oh, it's all up on my computer, like 200 open tabs.\n",
      "But yes, I'm not even sure if I want\n",
      "to pick my favorite honestly. Yeah, [INAUDIBLE]\n",
      "Maybe you can use a transformer like that [INAUDIBLE] The other one that I actually like even more is potentially, keep the context length fixed\n",
      "but allow the network to somehow use a scratch pad. And so the way this works is you will teach the transformer\n",
      "somehow via examples in [INAUDIBLE] hey, you actually have a scratch pad. Basically, you can't remember too much.\n",
      "Your context line is finite. But you can use a scratch pad. And you do that by emitting a start scratch pad, and then writing whatever you want to remember, and then\n",
      "end scratch pad. And then you continue with whatever you want. And then later when it's decoding,\n",
      "you actually have special objects that when you detect start scratch pad, you will like save whatever it puts\n",
      "in there in like external thing and allow it to attend over it. So basically, you can teach the transformer just dynamically\n",
      "because it's so meta-learned. You can teach it dynamically to use other gizmos and gadgets and allow it to expand its memory that way\n",
      "if that makes sense. It's just like human learning to use a notepad, right. You don't have to keep it in your brain.\n",
      "So keeping things in your brain is like the context line from the transformer. But maybe we can just give it a notebook. And then it can query the notebook, and read from it,\n",
      "and write to it. [INAUDIBLE] transformer to plug in another transformer. [LAUGHTER]\n",
      "[INAUDIBLE]\n",
      "I don't know if I detected that. I feel like-- did you feel like there was more than just a long prompt that's unfolding?\n",
      "Yeah, [INAUDIBLE]\n",
      "I didn't try extensively, but I did see a [INAUDIBLE] event. And I felt like the block size was just moved.\n",
      "Maybe I'm wrong. I don't actually know about the internals of ChatGPT. We have two online questions. So one question is, \"what do you think about architecture\n",
      "[INAUDIBLE]?\" S4? S4. I'm sorry.\n",
      "I don't know S4. Which one is this one? The second question, this one's a personal question.\n",
      "\"What are you going to work on next?\" [INAUDIBLE] I mean, so right now, I'm working on things like nanoGPT.\n",
      "Where is nanoGPT?\n",
      "I mean, I'm going basically slightly from computer vision and like computer vision-based products, do\n",
      "a little bit in language domain. Where's ChatGPT? OK, nanoGPT. So originally, I had minGPT, which I rewrote to nanoGPT.\n",
      "And I'm working on this. I'm trying to reproduce GPTs, and I mean, I think something like ChatGPT, I think,\n",
      "incrementally improved in a product fashion would be extremely interesting. And I think a lot of people feel it,\n",
      "and that's why it went so wide. So I think there's something like a Google plus\n",
      "plus plus to build that I think is more interesting. Shall we give our speaker a round of applause?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "# Set up Chrome options and service\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Runs Chrome in headless mode\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "service = Service(ChromeDriverManager().install())\n",
    "\n",
    "# Set up WebDriver\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Open YouTube video\n",
    "video_url = 'https://www.youtube.com/watch?v=XfpMkf4rD6E&list=PLpSmuCX65Aw8BbLqzdAB7aAcQR75BhMjz&index=43&t=589s'\n",
    "driver.get(video_url)\n",
    "\n",
    "# Wait for page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Scroll to the description section\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "time.sleep(2)\n",
    "\n",
    "# Expand the description if necessary (YouTube may hide part of the description)\n",
    "try:\n",
    "    more_button = driver.find_element(By.XPATH, '//*[@id=\"expand\"]')\n",
    "    more_button.click()\n",
    "    time.sleep(1)\n",
    "except Exception as e:\n",
    "    print(\"No 'More' button found or already expanded.\")\n",
    "\n",
    "# Wait for the transcript option to be visible and click it\n",
    "time.sleep(1)\n",
    "transcript_button = driver.find_element(By.XPATH, '//*[@id=\"primary-button\"]/ytd-button-renderer/yt-button-shape/button')\n",
    "transcript_button.click()\n",
    "\n",
    "# Wait for the transcript to load\n",
    "time.sleep(2)\n",
    "\n",
    "# Initialize an empty string to store the full transcript\n",
    "transcript = \"\"\n",
    "\n",
    "# Start from the first parent container and loop through all transcript segments\n",
    "index = 1\n",
    "while True:\n",
    "    try:\n",
    "        # Format the parent and child XPath using the index\n",
    "        parent_xpath = f'//*[@id=\"segments-container\"]/ytd-transcript-segment-renderer[{index}]'\n",
    "        child_xpath = f'{parent_xpath}/div/yt-formatted-string'\n",
    "\n",
    "        # Locate the child element that contains the transcript text\n",
    "        text_element = driver.find_element(By.XPATH, child_xpath)\n",
    "        transcript += text_element.text + \"\\n\"\n",
    "\n",
    "        # Move to the next parent container\n",
    "        index += 1\n",
    "    except Exception as e:\n",
    "        # Break the loop if no more segments are found\n",
    "        break\n",
    "\n",
    "# Print the entire transcript (text only)\n",
    "print(transcript)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e94506d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi, everyone. Welcome to CS 25 Transformers United V2. This was a course that was held at Stanford\\nin the winter of 2023. This course is not about robots that can transform into cars as this picture might suggest.\\nRather, it\\'s about deep learning models that have taken the world by storm and have revolutionized the field of AI and others.\\nStarting from natural language processing, transformers have been applied all over, computer vision, reinforcement learning, biology, robotics,\\net cetera. We have an exciting set of videos lined up for you with some truly fascinating speakers, talks, presenting\\nhow they\\'re applying transformers to the research in different fields and areas.\\nWe hope you\\'ll enjoy and learn from these videos. So without any further ado, let\\'s get started.\\nThis is a purely introductory lecture. And we\\'ll go into the building blocks of transformers.\\nSo first, let\\'s start with introducing the instructors. So for me, I\\'m currently on a temporary deferral from the PhD\\nprogram, and I\\'m leading AI at a robotics startup, Collaborative Robotics, that are working on some general purpose robots,\\nsomewhat like [INAUDIBLE]. And I\\'m very passionate about robotics and building FSG learning algorithms.\\nMy research interests are in reinforcement learning, computer vision, and remodeling, and I have a bunch of publications in robotics,\\nautonomous driving, and other areas. My undergrad was at Cornell. If someone is from Cornell, so nice to [INAUDIBLE]..\\nSo I\\'m Stephen, currently a first-year CS PhD here. Previously did my master\\'s at CMU and undergrad at Waterloo.\\nI\\'m mainly into NLP research, anything involving language and text, but more recently, I\\'ve\\nbeen getting more into computer vision as well as [INAUDIBLE] And just some stuff I do for fun, a lot of music\\nstuff, mainly piano. Some self-promo of what I post a lot on my Insta, YouTube, and TikTok, so if you guys want to check it out.\\nMy friends and I are also starting a Stanford piano club, so if anybody\\'s interested, feel free to email\\nor DM me for details. Other than that, martial arts, bodybuilding, and huge fan\\nof k-dramas, anime, and occasional gamer. [LAUGHS]\\nOK, cool. Yeah, so my name is Rylan. Instead of talking about myself, I just want to very briefly say that I\\'m super excited to take this class.\\nI took it the last time-- sorry-- to teach this. Excuse me. I took it the last time I was offered. I had a bunch of fun.\\nI thought we brought in a really great group of speakers last time. I\\'m super excited for this offering.\\nAnd yeah, I\\'m thankful that you\\'re all here, and I\\'m looking forward to a really fun quarter together. Thank you. Yeah, so fun fact, Rylan was the most outspoken student\\nlast year. And so if someone wants to become an instructor next year, you know what to do. [LAUGHTER]\\nOK, cool. Let\\'s see. OK, I think we have a few minutes.\\nSo what we hope you will learn in this class is, first of all, how do transformers work, how they\\nare being applied, just beyond NLP, and nowadays, like they are pretty [INAUDIBLE] them everywhere in AI machine learning.\\nAnd what are some new and interesting directions of research in these topics.\\nCool, so this class is just an introductory. So we\\'re just talking about the basics of transformers, introducing them, talking about the self-attention mechanism\\non which they\\'re founded. And we\\'ll do a deep dive more on models like BERT\\nto GPT, stuff like that. So with that, happy to get started. OK, so let me start with presenting the attention\\ntimeline. Attention all started with this one paper. [INAUDIBLE] by Vaswani et al in 2017.\\nThat was the beginning of transformers. Before that, we had the prehistoric error,\\nwhere we had models like RNM, LSDMs, and simple attention mechanisms that didn\\'t work\\nor [INAUDIBLE]. Starting 2017, we saw this explosion of transformers\\ninto NLP, where people started using it for everything. I even heard this quote from Google.\\nIt\\'s like our performance increased every time we [INAUDIBLE] [CHUCKLES]\\nFor the [INAUDIBLE] after 2018 to 2020, we saw this explosion of transformers into other fields like vision, a bunch of other stuff,\\nand like biology as a whole. And in last year, 2021 was the start of the generative era, where we got a lot of genetic modeling,\\nstarted models like Codex, GPT, DALL-E, stable diffusions, or a lot of things\\nhappening in genetic modeling. And we started scaling up in AI.\\nAnd now, the present. So this is 2022 and the startup in \\'23.\\nAnd now we have models like ChatGPT, Whisperer, a bunch of others.\\nAnd we\\'re scaling onwards without splitting up, so that\\'s great. So that\\'s the future.\\nSo going more into this, so once there were RNNs.\\nSo we had Seq2Seq models, LSTMs, GRU. What worked there was that they were good at encoding history,\\nbut what did not work was they didn\\'t encode long sequences and they were very bad at encoding context.\\nSo consider this example. Consider trying to predict the last word in the text,\\n\"I grew up in France, dot, dot, dot. I speak fluent Dutch.\" Here, you need to understand the context for it\\nto predict French, and attention mechanism is very good at that, whereas if they\\'re just using LSDMs,\\nit doesn\\'t here work that well. Another thing transformers are good at is,\\nmore based on content, is also context prediction is like finding attention maps.\\nIf I have something like a word like it, what noun does it correlate to.\\nAnd we can give a property attention on one of the possible activations.\\nAnd this works better than existing mechanisms.\\nOK, so where we were in 2021, we were on the verge of takeoff.\\nWe were starting to realize the potential of transformers in different fields. We solved a lot of long sequence problems\\nlike protein folding, AlphaFold, offline RL.\\nWe started to see few-shots, zero-shot generalization. We saw multimodal tasks and applications\\nlike generating images from language. So that was DALL-E. And it feels like [INAUDIBLE]..\\nAnd this was also a talk on transformers that you can watch on YouTube. Yeah, cool.\\nAnd this is where we were going from 2021 to 2022, which is we have gone from the version of [INAUDIBLE]\\nAnd now, we are seeing unique applications in audio generation, art, music, storytelling. We are starting to see these new capabilities\\nlike commonsense, logical reasoning, mathematical reasoning. We are also able to now get human enlightenment\\nand interaction. They\\'re able to use reinforcement learning and human feedback. That\\'s how ChatGPT is trained to perform really good.\\nWe have a lot of mechanisms for controlling toxicity bias and ethics now. And there are a lot of also, a lot\\nof developments in other areas like diffusion models. Cool.\\nSo the future is a spaceship, and we are all excited about it.\\nAnd there\\'s a lot of more applications that we can enable, and it\\'ll be great\\nif you can see transformers also up there. One big example is video understanding and generation.\\nThat is something that everyone is interested in, and I\\'m hoping we\\'ll see a lot of models in this area this year, also, finance, business.\\nI\\'ll be very excited to see GPT author a novel, but we need to solve very long sequence modeling.\\nAnd most transformer models are still limited to 4,000 tokens or something like that. So we need to make them generalize much more\\nbetter on long sequences. We also want to have generalized agents\\nthat can do a lot of multitask, a multi-input predictions\\nlike Gato. And so I think we will see more of that, too. And finally, we also want domain specific models.\\nSo you might want a GPT model, let\\'s put it like maybe your health. So that could be like a DoctorGPT model.\\nYou might have a LawyerGPT model that\\'s trained on only law data. So currently, we have GPT models that are trained on everything.\\nBut we might start to see more niche models that are good at one task. And we could have a mixture of experts,\\nso it\\'s like, you can think this is a-- how you\\'d normally consult an expert, you\\'ll have expert AI models.\\nAnd you can go to a different AI model for your different needs. There are still a lot of missing ingredients\\nto make this all successful. The first of all is external memory.\\nWe are already starting to see this with the models like ChatGPT, where the inflections are short-lived.\\nThere\\'s no long-term memory, and they don\\'t have ability to remember or store conversations for long-term.\\nAnd this is something you want to fix. Second is reducing the computation complexity.\\nSo attention mechanism is quadratic over the sequence length, which is slow. And we want to reduce it and make it faster.\\nAnother thing we want to do is we want to enhance the controllability of these models like a lot of these models can be stochastic.\\nAnd we want to be able to control what sort of outputs we get from them. And you might have experienced the ChatGPT,\\nif you just refresh, you get different output each time. But you might want to have a mechanism that controls what sort of things you get.\\nAnd finally, we want to align our state of art language models with how the human brain works.\\nAnd we are seeing the surge, but we still need more research on seeing how they can make more informed.\\nThank you. Great, hi. Yes, I\\'m excited to be here.\\nI live very nearby, so I got the invites to come to class. And I was like, OK, I\\'ll just walk over.\\nBut then I spent like 10 hours on the slides, so it wasn\\'t as simple. So yeah, I\\'m going to talk about transformers.\\nI\\'m going to skip the first two over there. I\\'m not going to talk about those. We\\'ll talk about that one just to simplify the lecture\\nsince we don\\'t have time. OK, so I wanted to provide a little bit of context\\non why does this transformers class even exist. So a little bit of historical context. I feel like Bilbo over there.\\nI joined like telling you guys about this. I don\\'t know if you guys saw Lord of the Rings.\\nAnd basically, I joined AI in roughly 2012, the full course, so maybe a decade ago.\\nAnd back then, you wouldn\\'t even say that you joined AI by the way. That was like a dirty word. Now, it\\'s OK to talk about, but back then, it\\nwas not even deep learning. It was machine learning. That was the term we would use if you were serious. But now, now, AI is OK to use, I think.\\nSo basically, do you even realize how lucky you are potentially entering this area in roughly 2023?\\nSo back then, in 2011 or so when I was working specifically on computer vision, your pipeline\\'s looked like this.\\nSo you wanted to classify some images, you would go to a paper, and I think this is representative. You would have three pages in the paper describing\\nall kinds of a zoo, of kitchen sink, of different kinds of features, descriptors. And you would go to a poster session\\nand in computer vision conference, and everyone would have their favorite feature descriptor that they\\'re proposing. And it\\'s totally ridiculous, and you\\nwould take notes on which one you should incorporate into your pipeline because you would extract all of them, and then you would put an SVM on top.\\nSo that\\'s what you would do. So there\\'s two pages. Make sure you get your [? Spar ?] SIFT histograms, your SSIMs, your color histograms, textiles,\\ntiny images. And don\\'t forget the geometry specific histograms. All of them have basically complicated code by themselves.\\nSo you\\'re collecting code from everywhere and running it, and it was a total nightmare. So on top of that, it also didn\\'t work.\\n[LAUGHTER] So this would be, I think, it represents the prediction from that time. You would just get predictions like this once in a while,\\nand you\\'d be like, you just shrug your shoulders like that just happens once in a while. Today, you would be looking for a bug.\\nAnd worse than that, every single chunk of AI\\nhad their own completely separate vocabulary that they work with. So if you go to NLP papers, those papers\\nwould be completely different. So you\\'re reading the NLP paper, and you\\'re like, what is this part of speech tagging,\\nmorphological analysis, and tactic parsing, co-reference resolution? What is MPBTKJ?\\nAnd you\\'re confused. So the vocabulary and everything was completely different. And you couldn\\'t read papers, I would say, across different areas.\\nSo now, that changed a little bit starting 2012 when Al Krizhevsky and colleagues basically\\ndemonstrated that if you scale a large neural network on large data set, you can get very strong performance.\\nAnd so up till then, there was a lot of focus on algorithms. But this showed that actually neural nets scale very well. So you need to now worry about compute and data,\\nand you can scale it up. It works pretty well. And then that recipe actually did copy paste across many areas of AI.\\nSo we start to see neural networks pop up everywhere since 2012. So we saw them in computer vision, and NLP, and speech,\\nand translation in RL and so on. So everyone started to use the same kind of modeling toolkit, modeling framework.\\nAnd now when you go to NLP, and you start reading papers there, in machine translation, for example, this is a sequence to sequence paper\\nwhich we\\'ll come back to in a bit. You start to read those papers, and you\\'re like, OK, I can recognize these words.\\nLike there\\'s a neural network. There\\'s some parameters. There\\'s an optimizer, and it starts to read things that you know of.\\nSo that decreased tremendously the barrier to entry across the different areas.\\nAnd then, I think, the big deal is that when the transformer came out in 2017, it\\'s not even that just the tool kits and the neural networks\\nwere similar-- there\\'s that literally the architectures converged to like one architecture that you copy paste across everything seemingly.\\nSo this was kind of an unassuming machine translation paper at the time, proposing to transformer architecture. But what we found since then is that you can just basically\\ncopy paste this architecture and use it everywhere. And what\\'s changing is the details of the data,\\nand the chunking of the data, and how you feed it in. And that\\'s a caricature, but it\\'s kind of like a correct first order statement.\\nAnd so now, papers are even more similar looking because everyone\\'s just using transformer. And so this convergence was remarkable to watch\\nand unfolded over the last decade. And it\\'s pretty crazy to me. What I find interesting is I think\\nthis is some kind of a hint that we\\'re maybe converging to something that maybe the brain is doing because the brain is very homogeneous and uniform\\nacross the entire sheet of your cortex. And OK, maybe some of the details are changing, but those feel like hyperparameters\\nlike a transformer. But your auditory cortex and your visual cortex and everything else looks very similar. And so maybe we\\'re converging to some kind\\nof a uniform powerful learning algorithm here. Something like that, I think, is interesting and exciting.\\nOK, so I want to talk about where the transformer came from briefly, historically. So I want to start in 2003.\\nI like this paper quite a bit. It was the first popular application of neural networks\\nto the problem of language modeling, so predicting in this case, the next word in the sequence, which allows you to build generative models over text.\\nAnd in this case, they were using multi-layer perceptron, so very simple neural net. The neural nets took three words and predicted the probability\\ndistribution for the fourth word in a sequence. So this was well and good at this point.\\nNow, over time, people started to apply this to machine translation. So that brings us to sequence to sequence paper\\nfrom 2014 that was pretty influential, and the big problem here was OK, we don\\'t just want to take three words and predict the fourth.\\nWe want to predict how to go from an English sentence to a French sentence. And the key problem was OK, you can\\nhave arbitrary number of words in English and arbitrary number of words in French, so how do you get an architecture that can process\\nthis variably sized input? And so here they used a LSDM, and there\\'s basically\\ntwo chunks of this, which are covered by the slack, by this.\\nBut basically have an encoder LSDM on the left, and it just consumes one word at a time\\nand builds up a context of what it has read. And then that acts as a conditioning vector to the decoder RNN or LSDM.\\nThat basically goes chonk, chonk, chonk for the next word in a sequence, translating the English to French or something like that.\\nNow, the big problem with this, that people identified, I think, very quickly and tried to resolve is that there\\'s what\\'s called this encoder bottleneck.\\nSo this entire English sentence that we are trying to condition on is packed into a single vector that goes from the encoder to the decoder.\\nAnd so this is just too much information to potentially maintain in a single vector, and that didn\\'t seem correct. And so people who are looking around for ways\\nto alleviate the attention of the encoder bottleneck as it was called at the time. And so that brings us to this paper,\\nNeural Machine Translation by Jointly Learning to Align and Translate. And here, just quoting from the abstract, \"in this paper,\\nwe conjectured that the use of a fixed length vector is a bottleneck in improving the performance of the basic encoder-decoder architecture\\nand propose to extend this by allowing the model to automatically soft search for parts of the source sentence that are relevant to predicting\\na target word without having to form these parts or hard segments exclusively.\"\\nSo this was a way to look back to the words that are coming from the encoder.\\nAnd it was achieved using this soft search. So as you are decoding in the words\\nhere, while you are decoding them, you are allowed to look back at the words at the encoder via this soft attention mechanism proposed\\nin this paper. And so this paper, I think, is the first time that I saw, basically, attention.\\nSo your context vector that comes from the encoder is a weighted sum of the hidden states\\nof the words in the encoding. And then the weights of this sum come\\nfrom a softmax that is based on these compatibilities between the current state as you\\'re decoding\\nand the hidden states generated by the encoder. And so this is the first time that really you start to look at it, and this is the current modern equations\\nof the attention. And I think this was the first paper that I saw it in. It\\'s the first time that there\\'s a word\\nattention used, as far as I know, to call this mechanism. So I actually tried to dig into the details of the history\\nof the attention. So the first author here, Dzmitry, I had an email correspondence with him,\\nand I basically sent him an email. I\\'m like, Dzmitry, this is really interesting. Just rumors have taken over. Where did you come up with the soft attention\\nmechanism that ends up being the heart of the transformer? And to my surprise, he wrote me back this massive email, which\\nwas really fascinating. So this is an excerpt from that email.\\nSo basically, he talks about how he was looking for a way to avoid this bottleneck between the encoder and decoder.\\nHe had some ideas about cursors that traverse the sequences that didn\\'t quite work out. And then here, \"so one day, I had this thought\\nthat it would be nice to enable the decoder RNN to learn to search where to put the cursor in the source sequence.\\nThis was sort of inspired by translation exercises that learning English in my middle school involved.\\nYour gaze shifts back and forth between source and target, sequence as you translate.\" So literally, I thought that this was kind of interesting,\\nthat he\\'s not a native English speaker, and here, that gave him an edge in this machine translation that led to attention and then led to transformer.\\nSo that\\'s really fascinating. \"I expressed a soft search a softmax and then weighted averaging of the [INAUDIBLE] states.\\nAnd basically, to my great excitement, this worked from the very first try.\" So really, I think, interesting piece of history.\\nAnd as it later turned out that the name of RNN search was kind of lame, so the better name attention came\\nfrom Yoshua on one of the final passes as they went over the paper. So maybe Attention is All You Need\\nwould have been called RNN Search is All You Need, but we have Yoshua Bengio to thank for a little bit of better name, I would say.\\nSo apparently, that\\'s the history of this, which I thought was interesting. OK, so that brings us to 2017, which is Attention\\nis All You Need. So this attention component, which in Dzmitry\\'s paper was just one small segment,\\nand there\\'s all this bidirectional RNN, RNN and decoder, and this Attention All You Need paper is saying,\\nOK, you can actually delete everything. What\\'s making this work very well is just attention by itself. And so delete everything, keep attention.\\nAnd then what\\'s remarkable about this paper actually is usually, you see papers that are very incremental. They add one thing, and they show that it\\'s better.\\nBut I feel like Attention is All You Need was like a mix of multiple things at the same time. They were combined in a very unique way,\\nand then also achieve a very good local minimum in the architecture space. And so to me, this is really a landmark paper\\nthat is quite remarkable and, I think, had quite a lot of work behind the scenes.\\nSo delete all the RNN, just keep attention. Because attention operates over sets-- and I\\'m going to go to this in a second--\\nyou now need to positionally encode your inputs because attention doesn\\'t have the notion of space by itself.\\nI have to be very careful. They adopted this residual network structure\\nfrom resonance. They interspersed attention with multi-layer perceptrons. They used layer norms, which came from a different paper.\\nThey introduced the concept of multiple heads of attention that were applied in parallel. And they gave us, I think, like a fairly good set\\nof hyperparameters that to this day are used. So the expansion factor in the multi-layer perceptron goes up\\nby 4X-- and we\\'ll go into a bit more detail-- and this 4X has stuck around. And I believe there\\'s a number of papers\\nthat try to play with all kinds of little details of the transformer, and nothing sticks because this is actually\\nquite good. The only thing to my knowledge that didn\\'t stick was this reshuffling of the layer norms\\nto go into the prenorm version where here you see the layer norms are after the multiheaded attention feed\\nforward. They just put them before instead. So just reshuffling of layer norms, but otherwise, the TPTs and everything else that you\\'re seeing today\\nis basically the 2017 architecture from 5 years ago. And even though everyone is working on it,\\nit\\'s been proven remarkably resilient, which I think is real interesting. There are innovations that, I think,\\nhave been adopted also in positional encoding. It\\'s more common to use different rotary and relative\\npositional encoding and so on. So I think there have been changes, but for the most part, it\\'s proven very resilient.\\nSo really quite an interesting paper. Now, I wanted to go into the attention mechanism.\\nAnd I think, the way I interpret it is not similar to the ways\\nthat I\\'ve seen it presented before. So let me try a different way of how I see it. Basically, to me, attention is kind of like the communication\\nphase of the transformer, and the transformer interweaves two phases of the communication phase, which\\nis the multi-headed attention, and the computation stage, which is this multilayered perceptron or [INAUDIBLE].\\nSo in the communication phase, it\\'s really just a data dependent message passing on directed graphs.\\nAnd you can think of it as OK, forget everything with machine translation, everything. Let\\'s just-- we have directed graphs.\\nAt each node, you are storing a vector. And then let me talk now about the communication\\nphase of how these vectors talk to each other and this directed graph. And then the compute phase later is just a multi-perceptron, which then basically acts on every node\\nindividually. But how do these nodes talk to each other in this directed graph?\\nSo I wrote like some simple Python-- I wrote this in Python basically to create\\none round of communication of using attention as the message passing scheme.\\nSo here, a node has this private data vector, as you can think of it as private information\\nto this node. And then it can also emit a key, a query, and a value. And simply, that\\'s done by linear transformation\\nfrom this node. So the key is what are the things that I am--\\nsorry. The query is what are the things that I\\'m looking for? The key is what other the things that I have? And the value is what are the things that I will communicate?\\nAnd so then when you have your graph that\\'s made up of nodes in some random edges, when you actually have these nodes communicating, what\\'s happening is\\nyou loop over all the nodes individually in some random order, and you\\'re at some node,\\nand you get the query vector q, which is, I\\'m a node in some graph, and this\\nis what I\\'m looking for. And so that\\'s just achieved via this linear transformation here. And then we look at all the inputs that point to this node,\\nand then they broadcast what are the things that I have, which is their keys. So they broadcast the keys.\\nI have the query, then those interact by dot product to get scores.\\nSo basically, simply by doing dot product, you get some unnormalized weighting of the interestingness of all of the information in the nodes\\nthat point to me and to the things I\\'m looking for. And then when you normalize that with softmax, so it just sums to 1, you basically just\\nend up using those scores, which now sum to 1 in our probability distribution, and you do a weighted sum of the values\\nto get your update. So I have a query. They have keys, dot products to get interestingness or like\\naffinity, softmax to normalize it, and then weighted sum of those values flow to me and update me.\\nAnd this is happening for each node individually. And then we update at the end. And so this kind of a message passing scheme\\nis at the heart of the transformer. And it happens in the more vectorized batched way\\nthat is more confusing and is also interspersed with layer norms and things like that to make the training behave\\nbetter. But that\\'s roughly what\\'s happening in the attention mechanism, I think, on a high level.\\nSo yeah, so in the communication phase of the transformer, then\\nthis message passing scheme happens in every head in parallel and then in every layer in series\\nand with different weights each time. And that\\'s it as far as the multi-headed attention goes.\\nAnd so if you look at these encooder-decoder models, you can think of it then in terms of the connectivity of these nodes in the graph.\\nYou can think of it as like, OK, all these tokens that are in the encoder that we want to condition on, they are fully connected to each other.\\nSo when they communicate, they communicate fully when you calculate their features. But in the decoder, because we are\\ntrying to have a language model, we don\\'t want to have communication for future tokens because they give away the answer at this step.\\nSo the tokens in the decoder are fully connected from all the encoder states, and then they\\nare also fully connected from everything that is decoding. And so you end up with this triangular structure\\nin the data graph. But that\\'s the message passing scheme that this basically implements.\\nAnd then you have to be also a little bit careful because in the cross attention here with the decoder, you consume the features from the top of the encoder.\\nSo think of it as in the encoder, all the nodes are looking at each other, all the tokens are looking at each other many, many times.\\nAnd they really figure out what\\'s in there, and then the decoder when it\\'s looking only at the top nodes.\\nSo that\\'s roughly the message passing scheme. I was going to go into more of an implementation of a transformer. I don\\'t know if there\\'s any questions about this.\\n[INAUDIBLE] self-attention and multi-headed attention, but what is the advantage of [INAUDIBLE]??\\nYeah, so self-attention and multi-headed attention, so the multi-headed attention is just this attention scheme,\\nbut it\\'s just applied multiple times in parallel. Multiple heads just means independent applications of the same attention.\\nSo this message passing scheme basically just happens in parallel multiple times with different weights for the query, key, and value.\\nSo you can almost look at it like in parallel, I\\'m looking for, I\\'m seeking different kinds of information from different nodes.\\nAnd I\\'m collecting it all in the same node. It\\'s all done in parallel. So heads is really just copy-paste in parallel.\\nAnd layers are copy-paste but in series.\\nMaybe that makes sense. And self-attention, when it\\'s self-attention,\\nwhat it\\'s referring to is that the node here produces each node here. So as I described it here, this is really self-attention\\nbecause every one of these nodes produces a key query and a value from this individual node. When you have cross-attention, you have one cross-attention\\nhere, coming from the encoder. That just means that the queries are still produced from this node, but the keys and the values\\nare produced as a function of nodes that are coming from the encoder.\\nSo I have my queries because I\\'m trying to decode some-- the fifth word in the sequence.\\nAnd I\\'m looking for certain things because I\\'m the fifth word. And then the keys and the values in terms of the source of information that could answer my queries\\ncan come from the previous nodes in the current decoding sequence or from the top of the encoder.\\nSo all the nodes that have already seen all of the encoding tokens many, many times cannot broadcast\\nwhat they contain in terms of information. So I guess, to summarize, the self-attention is--\\nsorry, cross-attention and self-attention only differ in where the piece and the values come from.\\nEither the keys and values are produced from this node, or they are produced from some external source like an encoder\\nand the nodes over there. But algorithmically, is the same mathematical operations.\\nQuestion. Yeah, OK. So two questions for you. First question is, in the message passing [INAUDIBLE]\\nSo think of-- so each one of these nodes is a token.\\nI guess they don\\'t have a very good picture of it in the transformer. But this node here could represent the third word\\nin the output in the decoder, and in the beginning, it is just the embedding of the word.\\nAnd then, OK, I have to think through this analogy a little bit more. I came up with it this morning.\\n[LAUGHTER] [INAUDIBLE]\\nWhat example of instantiation [INAUDIBLE] nodes\\nas in in blocks were embedding? These nodes are basically the vectors.\\nI\\'ll go to an implementation. I\\'ll go to the implementation, and then maybe I\\'ll make the connections to the graph.\\nSo let me try to first go to-- let me now go to, with this intuition in mind, at least, to a nanoGPT, which is a concrete implementation\\nof a transformer that is very minimal. So I worked on this over the last few days, and here it is reproducing GPT-2 on open web text.\\nSo it\\'s a pretty serious implementation that reproduces GPT-2, I would say, and provide it enough compute--\\nThis was one node of 8 GPUs for 38 hours or something like that, if I remember correctly. And it\\'s very readable.\\nIt\\'s 300 lines, so everyone can take a look at it. And yeah, let me basically briefly step through it.\\nSo let\\'s try to have a decoder-only transformer. So what that means is that it\\'s a language model.\\nIt tries to model the next word in the sequence or the next character in the sequence.\\nSo the data that we train on this is always some kind of text. So here\\'s some fake Shakespeare. Sorry, this is real Shakespeare.\\nWe\\'re going to produce fake Shakespeare. So this is called a Tiny Shakespeare dataset, which is one of my favorite toy datasets.\\nYou take all of Shakespeare, concatenate it, and it\\'s 1 megabyte file, and then you can train language models on it and get infinite Shakespeare, if you like,\\nwhich I think is kind of cool. So we have a text. The first thing we need to do is we need to convert it to a sequence of integers\\nbecause transformers natively process-- you can\\'t plug text into transformer.\\nYou need to somehow encode it. So the way that encoding is done is we convert, for example, in the simplest case, every character gets an integer, and then instead of \"hi\\nthere,\" we would have this sequence of integers. So then you can encode every single character as an integer\\nand get a massive sequence of integers. You just concatenate it all into one large, long one-dimensional sequence.\\nAnd then you can train on it. Now, here, we only have a single document. In some cases, if you have multiple independent documents,\\nwhat people like to do is create special tokens, and they intersperse those documents with those special end of text tokens\\nthat they splice in between to create boundaries. But those boundaries actually don\\'t have any modeling impact.\\nIt\\'s just that the transformer is supposed to learn via backpropagation that the end of document sequence means that you should wipe the memory.\\nOK, so then we produce batches. So these batches of data just mean that we go back to the one-dimensional sequence,\\nand we take out chunks of this sequence. So say, if the block size is 8, Then the block size indicates\\nthe maximum length of context that your transformer will process. So if our block size is 8, that means\\nthat we are going to have up to eight characters of context to predict the ninth character in a sequence.\\nAnd the batch size indicates how many sequences in parallel we\\'re going to process. And we want this to be as large as possible,\\nso we\\'re fully taking advantage of the GPU and the parallels [INAUDIBLE] So in this example, we\\'re doing a 4 by 8 batches.\\nSo every row here is independent example and then every row here is a small chunk of the sequence\\nthat we\\'re going to train on. And then we have both the inputs and the targets at every single point here.\\nSo to fully spell out what\\'s contained in a single 4 by 8 batch to the transformer-- I sort of compact it here--\\nso when the input is 47, by itself, the target is 58.\\nAnd when the input is the sequence 47, 58, the target is one. And when it\\'s 47, 58, 1, the target is 51 and so on.\\nSo actually, the single batch of examples that score by 8 actually has a ton of individual examples that we are expecting a transformer\\nto learn on in parallel. And so you\\'ll see that the batches are learned on completely independently, but the time dimension here along\\nhorizontally is also trained on in parallel. So your real batch size is more like B times T.\\nAnd it\\'s just that the context grows linearly for the predictions that you make along the T direction\\nin the model. So this is all the examples that the model will learn from, this single batch.\\nSo now, this is the GPT class. And because this is a decoder-only model,\\nso we\\'re not going to have an encoder because there\\'s no English we\\'re translating from-- we\\'re not trying to condition in some other external\\ninformation. We\\'re just trying to produce a sequence of words that follow each other or likely to.\\nSo this is all PyTorch, and I\\'m going slightly faster because I\\'m assuming people have taken 231 or something along those lines.\\nBut here in the forward pass, we take these indices, and then we both encode the identity of the indices,\\njust via an embedding lookup table. So every single integer, we index into a lookup table of\\nvectors in this, and end up embedding, and pull out the word vector for that token.\\nAnd then because the transformer by itself doesn\\'t actually-- the process is set natively.\\nSo we need to also positionally encode these vectors so that we basically have both the information about the token identity and its place in the sequence from 1\\nto block size. Now, the information about what and where is combined additively, so the token embeddings\\nand the positional embeddings are just added exactly as here. So then there\\'s optional dropout,\\nthis x here basically just contains the set of words and their positions,\\nand that feeds into the blocks of transformer. And we\\'re going to look into what\\'s block here. But for here, for now, this is just a series\\nof blocks in a transformer. And then in the end, there\\'s a layer norm, and then you\\'re decoding the logits\\nfor the next word or next integer in a sequence, using the linear projection of the output of this transformer\\nSo LM head here, a short core language model head. It\\'s just a linear function.\\nSo basically, positionally encode all the words, feed them into a sequence of blocks,\\nand then apply a linear layer to get the probability distribution for the next character.\\nAnd then if we have the targets, which we produced in the data order-- and you\\'ll notice that the targets are just\\nthe inputs offset by one in time-- then those targets feed into a cross entropy loss.\\nSo this is just a negative log likelihood typical classification loss. So now let\\'s drill into what\\'s here in the blocks.\\nSo these blocks that are applied sequentially, there\\'s, again, as I mentioned, this communicate phase and the compute phase.\\nSo in the communicate phase, all the nodes get to talk to each other, and so these nodes are basically,\\nif our block size is 8, then we are going to have eight nodes in this graph.\\nThere\\'s eight nodes in this graph. The first node is pointed to only by itself. The second node is pointed to by the first node and itself.\\nThe third node is pointed to by the first two nodes and itself, et cetera. So there\\'s eight nodes here.\\nSo you apply-- there\\'s a residual pathway and x. You take it out. You apply a layer norm, and then the self-attention\\nso that these communicate, these eight nodes communicate. But you have to keep in mind that the batch is 4. So because batch is 4, this is also applied--\\nso we have eight nodes communicating, but there\\'s a batch of four of them individually communicating in one of those eight nodes.\\nThere\\'s no crisscross across the batch dimension, of course. There\\'s no batch anywhere luckily. And then once they\\'ve changed information,\\nthey are processed using the multi-layer perceptron. And that\\'s the compute phase.\\nAnd then also here we are missing the cross-attention\\nbecause this is a decoder-only model. So all we have is this step here, the multi-headed attention, and that\\'s this line, the communicate phase.\\nAnd then we have the feed forward, which is the MLP, and that\\'s the compute phase.\\nI\\'ll take question\\'s a bit later. Then the MLP here is fairly straightforward.\\nThe MLP is just individual processing on each node, just transforming the feature representation at that node.\\nSo applying a two-layer neural net with a GELU nonlinearity, which is just\\nthink of it as a ReLU or something like that. It\\'s just a nonlinearity. And then MLP is straightforward.\\nI don\\'t think there\\'s anything too crazy there. And then this is the causal self-attention part, the communication phase.\\nSo this is like the meat of things and the most complicated part. It\\'s only complicated because of the batching\\nand the implementation detail of how you mask the connectivity in the graph so that you can\\'t obtain\\nany information from the future when you\\'re predicting your token. Otherwise, it gives away the information. So if I\\'m the fifth token and if I\\'m the fifth position,\\nthen I\\'m getting the fourth token coming into the input, and I\\'m attending to the third, second, and first,\\nand I\\'m trying to figure out what is the next token. Well then, in this batch, in the next element\\nover in the time dimension, the answer is at the input. So I can\\'t get any information from there.\\nSo that\\'s why this is all tricky, but basically, in the forward pass, we are calculating the queries, keys, and values based on x.\\nSo these are the keys, queries, and values. Here, when I\\'m computing the attention, I have the queries matrix multiplying the piece.\\nSo this is the dot product in parallel for all the queries and all the keys in all the heads.\\nSo I failed to mention that there\\'s also the aspect of the heads, which is also done all in parallel\\nhere. So we have the batch dimension, the time dimension, and the head dimension, and you end up with five-dimensional tensors, and it\\'s all really confusing.\\nSo I invite you to step through it later and convince yourself that this is actually doing the right thing. But basically, you have the batch dimension, the head\\ndimension and the time dimension, and then you have features at them. And so this is evaluating for all the batch elements, for all\\nthe head elements, and all the time elements, the simple Python that I gave you earlier, which is query\\ndot product p. Then here, we do a masked_fill, and what this is doing is it\\'s basically clamping the attention between the nodes\\nthat are not supposed to communicate to be negative infinity. And we\\'re doing negative infinity because we\\'re about to softmax, and so negative infinity will\\nmake basically the attention that those elements be zero. And so here we are going to basically end up\\nwith the weights, the affinities between these nodes, optional\\ndropout. And then here, attention matrix multiply v is basically\\nthe gathering of the information according to the affinities we calculated. And this is just a weighted sum of the values\\nat all those nodes. So this matrix multiplies is doing that weighted sum. And then transpose contiguous view\\nbecause it\\'s all complicated and batched in five-dimensional tensors, but it\\'s really not doing anything, optional drop out,\\nand then a linear projection back to the residual pathway. So this is implementing the communication phase here.\\nThen you can train this transformer. And then you can generate infinite Shakespeare.\\nAnd you will simply do this by-- because our block size is 8, we start with a sum token,\\nsay like, I used in this case, you can use something like a new line as the start token.\\nAnd then you communicate only to yourself because there\\'s a single node, and you get the probability distribution for the first word\\nin the sequence. And then you decode it for the first character in the sequence. You decode the character.\\nAnd then you bring back the character, and you re-encode it as an integer. And now, you have the second thing.\\nAnd so you get-- OK, we\\'re at the first position, and this is whatever integer it is, add the positional encodings,\\ngoes into the sequence, goes in the transformer, and again, this token now communicates with the first token and it\\'s identity.\\nAnd so you just keep plugging it back. And once you run out of the block size, which is eight, you start to crawl, because you can never\\nhave watt size more than eight in the way you\\'ve trained this transformer. So we have more and more context until eight. And then if you want to generate beyond eight,\\nyou have to start cropping because the transformer only works for eight elements in time dimension. And so all of these transformers in the [INAUDIBLE] setting\\nhave a finite block size or context length, and in typical models, this will be 1,024 tokens or 2,048\\ntokens, something like that. But these tokens are usually like BPE tokens, or SentencePiece tokens, or WorkPiece tokens.\\nThere\\'s many different encodings. So it\\'s not like that long. And so that\\'s why, I think, [INAUDIBLE].. We really want to expand the context size,\\nand it gets gnarly because the attention is sporadic in the [INAUDIBLE] case. Now, if you want to implement an encoder instead of a decoder\\nattention. Then all you have to do is this [INAUDIBLE] and you just delete that line.\\nSo if you don\\'t mask the attention, then all the nodes communicate to each other, and everything is allowed, and information\\nflows between all the nodes. So if you want to have the encoder here, just delete.\\nAll the encoder blocks will use attention where this line is deleted. That\\'s it. So you\\'re allowing whatever-- this encoder might store say,\\n10 tokens, 10 nodes, and they are all allowed to communicate to each other going up the transformer.\\nAnd then if you want to implement cross-attention, so you have a full encoder-decoder transformer, not just a decoder-only transformer or a GPT.\\nThen we need to also add cross-attention in the middle. So here, there is a self-attention piece where all\\nthe-- there\\'s a self-attention piece, a cross-attention piece, and this MLP. And in the cross-attention, we need\\nto take the features from the top of the encoder. We need to add one more line here, and this would be the cross-attention instead of a--\\nI should have implemented it instead of just pointing, I think. But there will be a cross-attention line here.\\nSo we\\'ll have three lines because we need to add another block. And the queries will come from x but the keys\\nand the values will come from the top of the encoder. And there will be basic code information\\nflowing from the encoder, strictly to all the nodes inside x. And then that\\'s it.\\nSo it\\'s a very simple modifications on the decoder attention. So you\\'ll hear people talk that you have\\na decoder-only model like GPT. You can have an encoder-only model like BERT, or you can have an encoder-decoder model\\nlike say T5, doing things like machine translation. And in BERT, you can\\'t train it using this language modeling\\nsetup that\\'s utter aggressive, and you\\'re just trying to predict next [INAUDIBLE] in the sequence. You\\'re training it doing slightly different objectives.\\nYou\\'re putting in the full sentence, and, the full sentence is allowed to communicate fully. And then you\\'re trying to classify sentiment or something\\nlike that. So you\\'re not trying to model the next token in the sequence. So these are trained slightly different\\nusing masking and other denoising techniques.\\nOK. So that\\'s like the transformer. I\\'m going to continue. So yeah, maybe more questions.\\n[INAUDIBLE]\\nThis is like we are enforcing these constraints on it by just masking [INAUDIBLE]\\nSo I\\'m not sure if I fully follow. So there\\'s different ways to look at this analogy, but one analogy is you can interpret\\nthis graph as really fixed. It\\'s just that every time we do the communicate, we are using different weights.\\nYou can look at it that way. So if we have block size of eight in my example, we would have eight nodes. Here we have 2, 4, 6.\\nOK, so we\\'d have eight nodes. They would be connected in-- you lay them out, and you only connect from left to right.\\n[INAUDIBLE]\\nWhy would they connect-- usually, the connections don\\'t change as a function of the data or something like that-- [INAUDIBLE]\\nI don\\'t think I\\'ve seen a single example where the connectivity changes dynamically in the function data. Usually, the connectivity is fixed.\\nIf you have an encoder, and you\\'re training a BERT, you have how many tokens you want, and they are fully connected.\\nAnd if you have a decoder-only model, you have this triangular thing, and if you have encoder-decoder, then you have\\nawkwardly two pools of nodes. Yeah.\\nGo ahead. [INAUDIBLE] I wonder, you know much more about this\\nthan I know. But do you have a sense of like if you ran [INAUDIBLE]\\nIn my head, I\\'m thinking [INAUDIBLE] but then you also\\nhave different things for one or more of [INAUDIBLE]---- Yeah, it\\'s really hard to say, so that\\'s\\nwhy I think this paper is so interesting because like, yeah, usually, you\\'d see like the path, and maybe they had path internally. They just didn\\'t publish it.\\nAll you can see is things that didn\\'t look like a transformer. I mean, you have ResNets, which have lots of this.\\nBut a ResNet would be like this, but there\\'s no self-attention component. But the MLP is there kind of in a ResNet.\\nSo a ResNet looks very much like this except there\\'s no-- you can use layer norms in ResNets, I believe, as well.\\nTypically, sometimes, they can be batch norms. So it is kind of like a ResNet. It is like they took a ResNet, and they\\nput in a self-attention block in addition to the preexisting MLP block, which is kind of like convolutions.\\nAnd MLP was strictly speaking deconvolution, one by one convolution, but I think\\nthe idea is similar in that MLP is just like a typical weights,\\nnonlinearity weights operation.\\nBut I will say, yeah, this is kind of interesting because a lot of work is not there, and then they give you this transformer.\\nAnd then it turns out 5 years later, it\\'s not changed, even though everyone\\'s trying to change it. So it\\'s interesting to me that it\\'s like a package,\\nin like a package, which I think is really interesting historically. And I also talked to paper authors,\\nand they were unaware of the impact that the transformer would have at the time. So when you read this paper, actually, it\\'s unfortunate\\nbecause this is the paper that changed everything, but when people read it, it\\'s like question marks because it reads like a pretty random machine translation\\npaper. It\\'s like, oh, we\\'re doing machine translation. Oh, here\\'s a cool architecture. OK, great, good results.\\nIt doesn\\'t know what\\'s going to happen. [LAUGHS] And so when people read it today, I think they\\'re confused potentially.\\nI will have some tweets at the end, but I think I would have renamed it with the benefit of hindsight of like, well, I\\'ll get to it.\\n[INAUDIBLE]\\nYeah, I think that\\'s a good question as well. Currently, I mean, I certainly don\\'t love the autoregressive modeling approach.\\nI think it\\'s kind of weird to sample a token and then commit to it. So maybe there are some ways, some hybrids\\nwith the Fusion as an example, which I think would be really cool, or we\\'ll find some other ways to edit the sequences later but still\\nin our regressive framework. But I think the Fusion is like an up and coming modeling\\napproach that I personally find much more appealing. When I sample text, I don\\'t go chunk, chunk, chunk, and commit.\\nI do a draft one, and then I do a better draft two. And that feels like a diffusion process.\\nSo that would be my hope. OK, also a question.\\nSo yeah, you\\'d think the [INAUDIBLE]\\nAnd then once we have the edge rates, we just have to multiply it by the values, and then you just [INAUDIBLE] it. Yes, yeah, it\\'s right.\\nAnd you think there\\'s MLG within graph neural networks and they\\'ll potentially--\\nI find the graph neural networks like a confusing term because, I mean, yeah, previously,\\nthere, was this notion of-- I feel like maybe today everything is a graph neural network because a transformer is a graph neural network\\nprocessor. The native representation that the transformer operates over is sets that are connected by edges in a direct way.\\nAnd so that\\'s the native representation, and then, yeah. OK, I should go on because I still have 30 slides.\\n[INAUDIBLE]\\nOh yeah, yeah, the root DE, I think, it basically like if you\\'re initializing with random weights\\nsetup from a [INAUDIBLE] as your dimension size grows, so does your values, the variance grows.\\nAnd then your softmax will just become the one half vector. So it\\'s just a way to control the variance\\nand bring it to always be in a good range for softmax and nice diffused distribution.\\nOK, so it\\'s almost like an initialization thing.\\nOK, so transformers have been applied to all the other fields, and the way this was done\\nis in my opinion, ridiculous ways honestly because I was a computer vision person,\\nand you have ComNets, and they make sense. So what we\\'re doing now with VITs as an example is you take an image and you chop it up into little squares.\\nAnd then those squares, literally, feed into a transformer, and that\\'s it, which is kind of ridiculous.\\nAnd so, I mean, yeah, and so the transformer doesn\\'t even, in the simplest case, really know where\\nthese patches might come from. They are usually positionally encoded, but it has to rediscover a lot of the structure,\\nI think, of them in some ways. And it\\'s kind of weird to approach it that way.\\nBut it\\'s just the simplest baseline of just chomping up big images into small squares and feeding them in as the individual nodes actually\\nworks fairly well. And then this is in a transformer encoder, so all the patches are talking to each other throughout the entire transformer.\\nAnd the number of nodes here would be like nine.\\nAlso, in speech recognition, you just take your melSpectrogram, and you chop it up into slices and you feed them into a transformer.\\nSo there was paper like this, but also Whisper. Whisper is a copy-paste transformer. If you saw Whisper from OpenAI, you just chop up melSpectrogram\\nand feed it into a transformer and then pretend you\\'re dealing with text. And it works very well.\\nDecision transformer in RL, you take your states, actions, and reward that you experience in environment, and you just pretend it\\'s a language.\\nThen you start to model the sequences of that, and then you can use that for planning later. That works really well.\\nEven things AlphaFold, so we were briefly talking about molecules and how you can plug them in. So at the heart of AlphaFold, computationally,\\nis also a transformer. One thing I wanted to also say about transformers is I find that they\\'re very flexible,\\nand I really enjoy that. I\\'ll give you an example from Tesla. You have a ComNet that takes an image\\nand makes predictions about the image. And then the big question is, how do you feed in extra information? And it\\'s not always trivial like say, I\\nhad additional information that I want to inform that I want the outputs to be informed by. Maybe I have other sensors like Radar.\\nMaybe I have some map information, or a vehicle type, or some audio. And the question is, how do you feed information into a ComNet?\\nLike where do you feed it in? Do you concatenate it? Do you add it? At what stage?\\nAnd so with a transformer, it\\'s much easier because you just take whatever you want, you chop it up into pieces, and you feed it in with a set\\nof what you had before. And you let the self-attention figure out how everything should communicate. And that actually apparently works.\\nSo just chop up everything and throw it into the mix is like the way. And it frees neural nets from this burgeon\\nof Euclidean space, where previously you had to arrange your computation to conform to the Euclidean\\nspace or three dimensions of how you\\'re laying out the compute. Like the compute actually kind of happens in almost like 3D space if you think about it.\\nBut in attention, everything is just sets. So it\\'s a very flexible framework, and you can just throw in stuff into your conditioning set.\\nAnd everything just self-attended over. So it\\'s quite beautiful from that perspective. OK, so now what exactly makes transformers so effective?\\nI think a good example of this comes from the GPT-3 paper, which I encourage people to read.\\nLanguage Models of Few-Shot Learners. I would have probably renamed this a little bit. I would have said something like transformers\\nare capable of in-context learning or meta-learning. That\\'s like what makes them really special.\\nSo basically the setting that they\\'re working with is, OK, I have some context, and I\\'m trying-- like say, a passage. This is just one example of many.\\nI have a passage, and I\\'m asking questions about it. And then as part of the context in the prompt,\\nI\\'m giving the questions and the answers. So I\\'m giving one example of question-answer, another example of question-answer, another example of question-answer, and so on.\\nAnd this becomes-- Oh yeah, people are going to have to leave soon, huh? OK, is this really important?\\nLet me think. OK, so what\\'s really interesting is basically\\nlike with more examples given in a context, the accuracy improves.\\nAnd so what that can set is that the transformer is able to somehow learn in the activations without doing any gradient descent\\nin a typical fine-tuning fashion. So if you fine-tune, you have to give an example and the answer, and you fine-tune it, using gradient descent.\\nBut it looks like the transformer internally in its weights is doing something that looks like potentially gradient, some kind of a metalearning in the weights of the transformer\\nas it is reading the prompt. And so in this paper, they go into, OK, distinguishing this outer loop with stochastic gradient\\ndescent in this inner loop of the intercontext learning. So the inner loop is as the transformer is reading the sequence almost and the outer loop is the training\\nby gradient descent. So basically, there\\'s some training happening in the activations of the transformer as it is consuming a sequence that\\nmay be very much looks like gradient descent. And so there are some recent papers that hint at this and study it.\\nAnd so as an example, in this paper here, they propose something called the draw operator. And they argue that the raw operator is implemented\\nby transformer, and then they show that you can implement things like ridge regression on top of the raw operator. And so this is giving--\\nThere are papers hinting that maybe there is some thing that looks like gradient-based learning inside the activations of the transformer.\\nAnd I think this is not impossible to think through because what is gradient-based learning? Overpass, backward pass, and then update.\\nOh, that looks like a ResNet, right, because you\\'re adding to the weights. So the start of initial random set of weights,\\nforward pass, backward pass, and update your weights, and then forward pass, backward pass, update the weights. Looks like a ResNet.\\nTransformer is a ResNet, so much more hand-wavey,\\nbut basically, some papers are trying to hint at why that would be potentially possible. And then I have a bunch of tweets I just copy-pasted here\\nin the end. This was like meant for general consumption, so they\\'re a bit more high-level and hypey a little bit.\\nBut I\\'m talking about why this architecture is so interesting and why potentially it became so popular.\\nAnd I think it simultaneously optimizes three properties that, I think, are very desirable. Number one, the transformer is very\\nexpressive in the forward pass. It sort of like it\\'s able to implement very interesting functions, potentially functions\\nthat can even do meta-learning. Number two, it is very optimizable thanks to things like residual connections, layer nodes,\\nand so on. And number three, it\\'s extremely efficient. This is not always appreciated, but the transformer, if you look at the computational graph,\\nis a shallow, wide network, which is perfect to take advantage of the parallelism of GPUs. So I think the transformer was designed very deliberately\\nto run efficiently on GPUs. There\\'s previous work like neural GPU that I really enjoy as well, which is really just\\nlike how do we design neural nets that are efficient on GPUs and thinking backwards from the constraints of the hardware, which I think is a very interesting way\\nto think about it.\\nOh yeah, so here, I\\'m saying, I probably would have called-- I probably would\\'ve called the transformer a general purpose\\nefficient optimizable computer instead of attention is all you need. That\\'s what I would have maybe in hindsight called that paper.\\nIt\\'s proposing a model that is very general purpose, so\\nforward passes, expressive. It\\'s very efficient in terms of GPU usage and is easily optimizable by gradient descent and trains\\nvery nicely. And then I have some other hype tweets here.\\nAnyway, so you can read them later. But I think this one is maybe interesting. So if previous neural nets are special purpose computers\\ndesigned for a specific task, GPT is a general purpose computer, reconfigurable at runtime\\nto run natural language programs. So the programs are given as prompts,\\nand then GPT runs the program by completing the document. So I really like these analogies personally to computer.\\nIt\\'s just like a powerful computer, and it\\'s optimizable by gradient descent.\\nAnd I don\\'t know--\\nOK, yeah. That\\'s it. [LAUGHTER] You can read the tweets later, but that\\'s for now. I\\'ll just thank you.\\nI\\'ll just leave this up.\\nSorry, I just found this tweet. So turns out that if you scale up the training set and use a powerful enough neural net like a transformer,\\nthe network becomes a kind of general purpose computer over text. So I think that\\'s nice way to look at it. And instead of performing a single text sequence,\\nyou can design the sequence in the prompt. And because the transformer is both powerful but also is trained on large enough, very hard data set,\\nit becomes this general purpose text computer. And so I think that\\'s kind of interesting way to look at it.\\nYeah. [INAUDIBLE]\\nAnd I guess my question is [INAUDIBLE] how much do you think [INAUDIBLE]?\\nreally because it\\'s mostly more efficient or [INAUDIBLE]\\nSo I think there\\'s a bit of that. Yeah, so I would say RNNs in principle, yes, they can implement arbitrary programs.\\nI think, it\\'s like a useless statement to some extent because they\\'re probably-- I\\'m not sure that they\\'re probably expressive\\nbecause in a sense of power and that they can implement these arbitrary functions.\\nBut they\\'re not optimizable. And they\\'re certainly not efficient because they are serial computing devices.\\nSo if you look at it as a compute graph, RNNs are very long, thin compute graph.\\nWhat if you stretched out the neurons and you looked-- like take all the individual neurons interconnectivity, and stretch them out, and try to visualize them.\\nRNNs would be like a very long graph and that\\'s bad. And it\\'s bad also for optimizability because I don\\'t exactly know why,\\nbut just the rough intuition is when you\\'re backpropagating, you don\\'t want to make too many steps. And so transformers are a shallow wide graph, and so\\nfrom supervision to inputs is a very small number of hops. And it\\'s a long residual pathways,\\nwhich make gradients flow very easily. And there\\'s all these layer norms to control the scales of all of those activations.\\nAnd so there\\'s not too many hops, and you\\'re going from supervision to input very quickly and just flows through the graph.\\nAnd it can all be done in parallel, so you don\\'t need to do this-- encoder and decoder RNNs, you have to go from first word,\\nthen second word, then third word. But here in transformer, every single word was processed completely in parallel, which is kind of a--\\nSo I think all of these are really important because all of these are really important. And I think number 3 is less talked about but extremely\\nimportant because in deep learning scale matters. And so the size of the network that you can train it\\ngives you is extremely important. And so if it\\'s efficient on the current hardware, then you can make it bigger.\\nYou mentioned that if you do it with multiple modalities of data, [INAUDIBLE].\\nHow does that actually work? Do you leave the different data as different token, or is it [INAUDIBLE]?\\nNo, so yeah, so you take your image, and you apparently chop them up into patches. So there\\'s the first thousand tokens or whatever.\\nAnd now, I have a special-- so radar could be also, but I don\\'t actually\\nwant to make a representation of radar. But you just need to chop it up and enter it.\\nAnd then you have to encode it somehow. Like the transformer needs to know that they\\'re coming from radar. So you create a special--\\nyou have some kind of a special token of that to-- these radar tokens are what\\'s slightly different in the representation, and it\\'s\\nlearnable by gradient descent. And like vehicle information would also come in with a special embedded token that can be learned.\\nSo-- So how do you line those before really-- Actually, but you don\\'t. It\\'s all just a set.\\nAnd there\\'s-- Even the [INAUDIBLE] Yeah, it\\'s all just a set, but you can positionally\\nencode these sets if you want. So positional encoding means you can\\nhardwire, for example, the coordinates like using [INAUDIBLE]. You can hardwire that, but it\\'s better\\nif you don\\'t hardwire the position. It\\'s just a vector that is always hanging out the dislocation. Whatever content is there, it just adds on it.\\nAnd this vector is trainable by background. That\\'s how you do it.\\nGood point. I don\\'t really like the [INAUDIBLE]..\\nThey seem to work, but it seems like they\\'re sometimes [INAUDIBLE]\\nI\\'m not sure if I understand your question. [LAUGHTER] So I mean the positional encoders like they\\'re actually like not--\\nOK, so they have very little inductive bias or something like that. They\\'re just vectors hanging out in location always,\\nand you\\'re trying to help the network in some way. And I think the intuition is good,\\nbut if you have enough data, usually, trying to mess with it is a bad thing. Trying to enter knowledge when you\\nhave enough knowledge in the data set itself is not usually productive. So it all really depends on what scale you want. If you have infinity data, then you actually\\nwant to encode less and less. That turns out to work better. And if you have very little data, then actually, you do want to encode some biases.\\nAnd maybe if you have a much smaller data set, then maybe convolutions are a good idea because you actually have this bias coming from your filters.\\nBut I think-- so the transformer is extremely general, but there are ways to mess with the encodings\\nto put in more structure. Like you could, for example, encode [INAUDIBLE] and fix it, or you could actually go to the attention mechanism\\nand say, OK, if my image is chopped up into patches, this patch can only communicate to this neighborhood.\\nAnd you just do that in the attention matrix, you just mask out whatever you don\\'t want to communicate.\\nAnd so people really play with this because the full attention is inefficient. So they will intersperse, for example, layers\\nthat only communicate in little patches and then layers that communicate globally. And they will do all kinds of tricks like that.\\nSo you can slowly bring in more inductive bias. You would do it, but the inductive biases are like they\\'re factored out from the core transformer.\\nAnd they are factored out, and the interconnectivity of the nodes. And they are factored out in the positionally--\\nand you can mess with this for computation. [INAUDIBLE]\\nSo there\\'s probably about 200 papers on this now if not more. They\\'re kind of hard to keep track of.\\nHonestly, like my Safari browser, which is-- oh, it\\'s all up on my computer, like 200 open tabs.\\nBut yes, I\\'m not even sure if I want\\nto pick my favorite honestly. Yeah, [INAUDIBLE]\\nMaybe you can use a transformer like that [INAUDIBLE] The other one that I actually like even more is potentially, keep the context length fixed\\nbut allow the network to somehow use a scratch pad. And so the way this works is you will teach the transformer\\nsomehow via examples in [INAUDIBLE] hey, you actually have a scratch pad. Basically, you can\\'t remember too much.\\nYour context line is finite. But you can use a scratch pad. And you do that by emitting a start scratch pad, and then writing whatever you want to remember, and then\\nend scratch pad. And then you continue with whatever you want. And then later when it\\'s decoding,\\nyou actually have special objects that when you detect start scratch pad, you will like save whatever it puts\\nin there in like external thing and allow it to attend over it. So basically, you can teach the transformer just dynamically\\nbecause it\\'s so meta-learned. You can teach it dynamically to use other gizmos and gadgets and allow it to expand its memory that way\\nif that makes sense. It\\'s just like human learning to use a notepad, right. You don\\'t have to keep it in your brain.\\nSo keeping things in your brain is like the context line from the transformer. But maybe we can just give it a notebook. And then it can query the notebook, and read from it,\\nand write to it. [INAUDIBLE] transformer to plug in another transformer. [LAUGHTER]\\n[INAUDIBLE]\\nI don\\'t know if I detected that. I feel like-- did you feel like there was more than just a long prompt that\\'s unfolding?\\nYeah, [INAUDIBLE]\\nI didn\\'t try extensively, but I did see a [INAUDIBLE] event. And I felt like the block size was just moved.\\nMaybe I\\'m wrong. I don\\'t actually know about the internals of ChatGPT. We have two online questions. So one question is, \"what do you think about architecture\\n[INAUDIBLE]?\" S4? S4. I\\'m sorry.\\nI don\\'t know S4. Which one is this one? The second question, this one\\'s a personal question.\\n\"What are you going to work on next?\" [INAUDIBLE] I mean, so right now, I\\'m working on things like nanoGPT.\\nWhere is nanoGPT?\\nI mean, I\\'m going basically slightly from computer vision and like computer vision-based products, do\\na little bit in language domain. Where\\'s ChatGPT? OK, nanoGPT. So originally, I had minGPT, which I rewrote to nanoGPT.\\nAnd I\\'m working on this. I\\'m trying to reproduce GPTs, and I mean, I think something like ChatGPT, I think,\\nincrementally improved in a product fashion would be extremely interesting. And I think a lot of people feel it,\\nand that\\'s why it went so wide. So I think there\\'s something like a Google plus\\nplus plus to build that I think is more interesting. Shall we give our speaker a round of applause?\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8dceb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
